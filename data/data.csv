id,title,abstract,authors-names,author-emails,primary-contact,track,authors,primary-subject,secondary-subject,type,format,presence,duration,notes,session_code,session_name,paper_url,video_url,slides_url,image_url,image_credit,keywords,problems
138,Visually-Led Design for Gestural Audiovisual Instruments,"In this paper we present our visually-led design method for creating gestural mappings in a new audiovisual percussion work titled Cymbalism. Unlike most audiovisual works, Cymbalism was inspired by the creation of a series of interactive visual scenes that respond to the performer’s real-time movements. In leading with the visual interaction, we discuss how this approach fostered a union between the physical, audio and visual elements of the work, creating a performance where the visualisation is not simply a feedback mechanism but fundamental in inspiring compositional concepts and new ways of interacting with sound. Through practice-based research, we use the insights gained through creative development and performance outcomes to guide the continued evolution of an established wearable gestural DMI.",Sam Trolland (Monash)*; Alon Ilsar (Monash); Jon McCormack (Monash),sam.trolland@monash.edu; alon.ilsar@monash.edu; Jon.McCormack@monash.edu,sam.trolland@monash.edu,paper,Sam Trolland; Alon Ilsar; Jon McCormack,"Explorations of relationships between motion, gesture and music","Extended reality environments: augmented, virtual, mixed reality; Gesture to sound mapping; Musical mapping strategies",long,oral,in person,15,,papers-1,Collective and Embodied,nime2025_138.pdf,,,138.jpg,,"Gesture, Audio-Visual, Instrument, Percussion, Performance, Visual",
323,Drawing Space with Rain: The Umbrella as a Flow Interface,"This study explores new possibilities for transforming perceived space by using an umbrella as a dynamic spatial auditory interface. While spatial audio technologies have been widely applied across various domains, there are few opportunities in daily life to consciously perceive the boundary between personal and external space. Due to its physical structure and everyday usage, the umbrella has a unique ability to render such boundaries perceptible. Focusing on the “flow” of raindrops across the umbrella’s surface, the system detects continuous rain movement in real time rather than merely capturing impact sounds. Spatial auditory feedback encourages users to actively perceive dynamic spatial boundaries, as the rain draws auditory contours through interaction with the umbrella. To this end, the umbrella is conceptualized as an interactive interface that senses raindrop movement and applies spatial audio processing. In addition, users can dynamically alter the virtual size of the umbrella, enabling perceptual shifts in spatial scale. Rather than treating the umbrella as a mere protective object, this system reimagines it as a medium for perceiving environmental change through sound. By integrating natural phenomena with spatial audio, this approach suggests new directions for embodied perception and expression.",Kana Yamaguchi (University of Tsukuba)*; Yuga Tsukuda (University of Tsukuba); Yoichi Ochiai (University of Tsukuba),kana.yamaguchi@digitalnature.slis.tsukuba.ac.jp; tsukuda@digitalnature.slis.tsukuba.ac.jp; wizard@slis.tsukuba.ac.jp,kana.yamaguchi@digitalnature.slis.tsukuba.ac.jp,paper,Kana Yamaguchi; Yuga Tsukuda; Yoichi Ochiai,"Discussions about the artistic, cultural, and social impact of NIME technology",Interactive sound art and sound installations,long,oral,in person,15,,papers-1,Collective and Embodied,nime2025_323.pdf,,,323.jpg,,"Umbrella, Rain, Sensing flow, Spatial Audio, Augmented Spatial Perception",
50,Drum Modal Feedback: Concept Design of an Augmented Percussion Instrument,"We here outline the concept design of an augmented percussion instrument, conceived for and used as part of a variety of distinct performances and compositions. Throughout the curation of this project, each creative act has enabled us to contextualise, examine and reflect upon the design of this augmented instrument. In accordance with Stolterman and Wiberg’s concept driven design methodology, we do not present a singular instrument design, but instead an overarching design concept alongside its developmental and evaluative narrative. This augmentation centres upon the use of a drum trigger and a tactile transducer, which when coupled together can be used to feedback or resonate a drum. The resultant soundworld develops upon the idiomatic sonority of a drum, and allows for the duration and timbre of a drum strike to be continuously manipulated and shaped throughout a performance. In exploring the soundworld which results from this approach, we have experimented with numerous configurations of these pieces of hardware, and have also employed various pieces of software to parametrise the sonic subtleties that this approach engenders. Most prominently, we have developed a bespoke piece of software which analyses the modes of a drum prior to performance, and uses this modal analysis to shape the overall feedback and resonance. Throughout this design process, we have consistently been met with new creative criteria that challenge our approach and ideas, in response to the particularities of the musicians we are working alongside, as well as the performative and aesthetic environments we are working within.",Lewis Wolstanholme (Queen Mary University of London)*; Jordie Shier (Queen Mary University of London); Rodrigo Constanzo (Royal Northern College of Music); Andrew McPherson (Imperial College London),l.wolstanholme@qmul.ac.uk; j.m.shier@qmul.ac.uk; rodrigo.constanzo@rncm.ac.uk; andrew.mcpherson@imperial.ac.uk,l.wolstanholme@qmul.ac.uk,paper,Lewis Wolstanholme; Jordie Shier; Rodrigo Constanzo; Andrew McPherson,"Augmented, embedded and hyper instruments",Practice-based research approaches/methodologies/criticism,long,oral,in person,15,,papers-2,Novel Techniques and Technologies,nime2025_50.pdf,,,50.jpg,,"augmented percussion instruments, concept design, practice based reasearch",
115,"Evolving the Living Looper: Artistic Research, Online Learning, and Tentacle Pendula","The Living Looper is a neural audio synthesis looper system for live input. It combines online learning with pre-trained neural network models to resynthesize incoming audio into ""living loops"" that transform over time. This paper describes new features of the Living Looper and musician perspectives on its use. A new graphical interface facilitates use of the instrument by non-programmers and visualizes each loop to aid performers in tracking which loop is making which sound.  We also describe a new living loop algorithm including incremental learning with partial least squares regression. Finally, we report on an artistic project using the Looper and lessons learned, resulting in an increased importance of training data and a developing sense of relationality.",Victor Shepardson (Intelligent Instruments Lab)*; Halla Steinunn Stefánsdóttir (Intelligent Instruments Lab); Thor Magnusson (Intelligent Instruments Lab),victorshepardson@hi.is; hallasteinunn@hi.is; thormagnusson@hi.is,victorshepardson@hi.is,paper,Victor Shepardson; Halla Steinunn Stefánsdóttir; Thor Magnusson,"Novel controllers, interfaces or instruments for musical expression","Augmented, embedded and hyper instruments ; Machine learning and artificial intelligence in NIMEs ; Machine learning in musical performance",medium,oral,in person,10,,papers-2,Novel Techniques and Technologies,nime2025_115.pdf,,,115.jpg,,"software,machine learning,neural synthesis,looper",
147,Audiation Development System for Gugak's Fluid Musical Parameters Utilizing Audio Feedback Stimuli,"Gugak, the traditional music of Korea, is defined by its distinctive musical characteristics, including flexible tuning, non-metric rhythms, and intricate ornamentation. These unique features, while artistically rich, pose significant challenges for novice learners, particularly when approached through conventional, textbook-based methods. To bridge this gap, we introduce the Audiation Development System for Gugak, an interactive platform that leverages algorithmic analysis to support both learning and teaching. Central to this system is the development of signal processing functions for rhythmic guidance, pitch detection, and additive harmonic synthesis—algorithms specifically designed to capture the expressive nuances of Gugak. These functions generate real-time auditory and visual feedback, providing a responsive learning environment aligned with the updated Korean music curriculum. The system not only enables student-centred exploration of Gugak's fluid structures but also supports educators through dynamic, visualized feedback tools. Beyond its technical foundation, this research sets the stage for future development of user interfaces and investigates the educational efficacy of computer-driven learning compared to traditional methods. By integrating music technology and pedagogy, this work contributes to both the accessibility and sustainability of Korea’s musical heritage.",Michaella Moon (Victoria University of Wellington)*; Dale Carnegie (Victoria University of Wellington); Jim Murphy (	Victoria University of Wellington),michaella.moon@vuw.ac.nz; dale.carnegie@vuw.ac.nz; jim.murphy@vuw.ac.nz,michaella.moon@vuw.ac.nz,paper,Michaella Moon; Dale Carnegie; Jim Murphy,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Historical, theoretical or philosophical discussions about designing or performing with new interfaces ; Music-related human-computer interaction",long,oral,in person,15,,papers-2,Novel Techniques and Technologies,nime2025_147.pdf,,,147.jpg,,"Korean-traditional music genre, Gugak educational controllers, Audio stimuli, Feedback system",
161,Threading the Sound: The Carpet Tufting Gun as an Electroacoustic Performance Interface,"This paper explores the carpet tufting gun as a novel electroacoustic performance interface. Leveraging its distinctive acoustic properties and electromechanical kinetics, the tufting gun presents a range of physical affordances that can be creatively repurposed for musical expression. While prior intersections between textile production processes and musical practices exist, the tufting gun remains largely underexplored as a tool for structured musical composition. This work reimagines the gun’s mechanical gestures and performative affordances, transforming its utilitarian motions into expressive sonic gestures. By positioning the tufting gun as both an acoustic source and an interactive performance interface, this project works at the intersection of fibre craft and experimental sound art, where both historico-cultural context of textile making, and the ergonomics of the gun, present musical affordances.",Joseph Burgess (University of the Sunshine Coast)*; Toby Gifford (University of the Sunshine Coast),unregisteredmasterbuilder@gmail.com; tgifford@usc.edu.au,unregisteredmasterbuilder@gmail.com,paper,Joseph Burgess; Toby Gifford,"Novel controllers, interfaces or instruments for musical expression",Musical mapping strategies ; New music performance paradigms; Sonic interaction design,short,oral,in person,5,,papers-2,Novel Techniques and Technologies,nime2025_161.pdf,,,161.jpg,,"Carpet, Tufting, Electroacoustic, Interfaces",
198,Designing Percussive Timbre Remappings: Negotiating Audio Representations and Evolving Parameter Spaces,"Timbre remapping is an approach to audio-to-synthesizer parameter mapping that aims to transfer timbral expressions from a source instrument onto synthesizer controls. This process is complicated by the ill-defined nature of timbre and the complex relationship between synthesizer parameters and their sonic output. In this work, we focus on real-time timbre remapping with percussion instruments, combining technical development with practice-based methods to address these challenges. As a technical contribution, we introduce a genetic algorithm -- applicable to black-box synthesizers including VSTs and modular synthesizers -- to generate datasets of synthesizer presets that vary according to target timbres. Additionally, we propose a neural network-based approach to predict control features from short onset windows, enabling low-latency performance and feature-based control. Our technical development is grounded in musical practice, demonstrating how iterative and collaborative processes can yield insights into open-ended challenges in DMI design. Experiments on various audio representations uncover meaningful insights into timbre remapping by coupling data-driven design with practice-based reflection. This work is accompanied by an annotated portfolio, presenting a series of musical performances and experiments with reflections.",Jordan Shier (Queen Mary University of London)*; Rodrigo Constanzo (Royal Nothern College of Music); Charalampos Saitis (Queen Mary University of London); Andrew Robertson (Ableton AG); Andrew McPherson (Imperial College London),j.m.shier@qmul.ac.uk; rodrigo.constanzo@rncm.ac.uk; c.saitis@qmul.ac.uk; andrew.robertson@ableton.com; andrew.mcpherson@imperial.ac.uk,j.m.shier@qmul.ac.uk,paper,Jordan Shier; Rodrigo Constanzo; Charalampos Saitis; Andrew Robertson; Andrew McPherson,Gesture to sound mapping,"Augmented, embedded and hyper instruments ; Machine learning and artificial intelligence in NIMEs",long,oral,in person,15,,papers-2,Novel Techniques and Technologies,nime2025_198.pdf,,,198.jpg,,"synthesizer parameter mapping, timbre remapping, machine learning, practice-led research,",
129,Aquapella- Gestural Interactions with Liquid Turbulence as Musical Expression,"The Aquapella is a hand-held gestural instrument for exploring the unique relationship of liquid turbulence and musical expression. The device consists of eight conductive water-level sensors in a custom 3D printed container. As a musician moves the device, it generates a chaotic flow of water within the container and translates the motion into real-time midi signals for audio-visual interpretation. In our initial performances and tests with the Aquapella, we have focused on turning the flowing characteristics of the device into ambient and glitch soundscapes that move between noise and harmonics. We present the primary findings in developing the Aquapella including related works, description of the project development, and ideas for future iterations.",John Lettang (University of Colorado Boulder)*; August Black (University of Colorado Boulder),john.lettang@colorado.edu; augustblack@gmail.com,john.lettang@colorado.edu,paper,John Lettang; August Black,"Novel controllers, interfaces or instruments for musical expression","Explorations of relationships between motion, gesture and music; Gesture to sound mapping; Sensor and actuator technologies, including haptics and force feedback devices",long,oral,in person,15,,papers-3,Body and Motion,nime2025_129.pdf,,,129.jpg,,"Water Instrument, Gestural Instrument, Tangible Interface",
10,Looping slowly: Diffraction through the lens of nostalgia,"This paper concerns magnetic tape and the nostalgia of media, finding new relevance in old technology, remaking and adapting practices to fit within a modern workflow. Pushing against the driving force of economic structures, which emphasises a continuous cycle of replacement, musicians and instrument designers are drawing on a shared history to create new pieces of art and machines. This can be read as reflecting NIME's Code of Practice and, more generally, the unfolding climate crisis. For some, NIME may convey a focus on new musical instruments, but here, we focus on the notion of new through the diffracted lens of the old. Defined in recent NIME conferences by zooming in on the 'O' in NIME through the importance of reusing and repurposing old musical instruments and, in our case, old practices and processes. This paper considers magnetic tape and the machines that process it as the material and instrument. Following a survey, we present a diffracted reading through an intra-related process of how musicians, producers, and others who work with audio integrate tape into their practice. Drawing on post-humanist theories, we explore how slowness, community, and the old can inform NIME as a methodology. It provides insight for NIME to continue moving forward while focusing, through its Code of Practice, on sustainability, connection with our past, our history, years of artistic practice, and workflows that are not simply optimised for efficiency or the new.",Benedict Gaster (Univeristy of West England)*; Nathan Renney (University of West of England); Jasmine Butt (University of West of England),benedict.gaster@uwe.ac.uk; Nathan.Renney@uwe.ac.uk; jasmine2.butt@live.uwe.ac.uk,benedict.gaster@uwe.ac.uk,paper,Benedict Gaster; Nathan Renney; Jasmine Butt,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Music-related human-computer interaction",long,oral,in person,15,,papers-4,"Environment, Sustainability, Longevity",nime2025_10.pdf,,,10.jpg,,"Tape, nostalgia, art, agency, ethnography, design",
96,Embodying Sustainability: Paving Opportunities for NIME Research,"While sustainability has gained attention in NIME research, primarily focusing on instrument longevity and durability, the role musical interfaces play in promoting environmental awareness remains unexplored. This paper investigates how musical interfaces can foster sustainability through designing embodied experiences. We present a literature review that examines the integration of sustainability and embodiment in sonic interaction, in which we synthesize practical points on how sound, materials, data, and interactions can aesthetically support embodying sustainability. We further explore these concepts through a design case study. Our findings suggest that embodied musical experiences offer unique opportunities to cultivate environmental consciousness, contributing to a deeper understanding of sustainable musical interfaces relying on artistic expressions.","Xinran Chen (The Hong Kong University of Science and Technology (Guangzhou))*; Iurii Kuzmin (The Hong Kong University of Science and Technology); Mela Bettega (Open Lab, School of Computing, Newcastle University); Raul Masu (The Hong Kong University of Science and Technology (Guangzhou))",xchen805@connect.hkust-gz.edu.cn; ikuzmin@connect.ust.hk; mela.bettega@newcastle.ac.uk; raul@raulmasu.org,xchen805@connect.hkust-gz.edu.cn,paper,Xinran Chen; Iurii Kuzmin; Mela Bettega; Raul Masu,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Music-related human-computer interaction ; Sonic interaction design",long,oral,in person,15,,papers-4,"Environment, Sustainability, Longevity",nime2025_96.pdf,,,96.jpg,,"Embodiment, Sustainability, Environmental awareness, Aesthetics, Sustainability through design (StD)",
153,Ongoing Production of a “Growing Instrument” Using Mycelium-Based Materials,"This study explores “growing instruments” made from fungal mycelium, highlighting their natural unpredictability and role as musical instruments. Mycelium’s growth and interactions with the environment create unique features not found in traditional instruments. Positioned within non-human-centric approaches in design and art, the research emphasizes the mutual creation and interconnectedness of diverse actors. By shaping mycelium into tubes and adding recorder heads, playable flute-like instruments were created. However, their condition and playability were highly influenced by conditions such as temperature and humidity. The study emphasizes embracing uncertainty and suggests that these imperfections can offer new insights into musical instrument design.",Taisei GOTO (Kyushu University)*; Kazuhiro JO (Kyushu University),goto.taisei.703@s.kyushu-u.ac.jp; jo@jp.org,goto.taisei.703@s.kyushu-u.ac.jp,paper,Taisei GOTO; Kazuhiro JO,Practice-based research approaches/methodologies/criticism,"Historical, theoretical or philosophical discussions about designing or performing with new interfaces",short,oral,in person,5,,papers-4,"Environment, Sustainability, Longevity",nime2025_153.pdf,,,153.jpg,,"Mycelium, Growing Instruments, Non-Human-Centered Design, Uncertainty, Musical Instrument Design",
39,"Synthetic Ornithology: Machine learning, simulations and hyper-real soundscapes","This paper presents Synthetic Ornithology, an interactive sound-based installation that uses machine learning to simulate sonic representations of localised Australian ecological futures, extending work in soundscape composition to engage in a speculative domain. Central to Synthetic Ornithology is a bespoke ML model, Environmental Audio Generation for Localised Ecologies (EAGLE), capable of generating high-quality, birdsong-focused soundscapes, up to 23 seconds in length. This paper outlines the development of the installation and how its design aims to influence audience perception of the sonic content of the work, extending established practices in NIME and sonic arts to a parafictional approach, and hyperreal aesthetics. Additionally, the paper examines the design and capabilities of the EAGLE model, and reflecting on how generative tools are positioned within a creative context, re-imagines the technical processes of training and configuring ML models as sites of artistic authorship in an expanded creative audio practice.",Frederick Rodrigues (Deakin University)*,s222405968@deakin.edu.au,s222405968@deakin.edu.au,paper,Frederick Rodrigues,Machine learning and artificial intelligence in NIMEs,Interactive sound art and sound installations ; Sonic interaction design,long,oral,in person,15,,papers-5,Machine Learning and Co-Creativity,nime2025_39.pdf,,,39.jpg,,"soundscapes, machine learning, climate change, generative audio",
131,Touching Wires: tactility and a quilted musical interface for human-AI musical co-creation,"Interactions with computers have traditionally been mediated by rigid materials, but as technology evolves, there is increasing potential to rethink these relationships. This paper explores how a soft, textile-based interface can reshape human-AI interaction, particularly in musical co-creation. We introduce a textile-based human-AI system used both for musical performance and public interaction. This system enables embodied, tactile engagement with an AI agent, offering users a more unique and participatory experience in human-AI musical co-creation. We aim to examine the potential for soft materiality to mediate more dynamic human-AI interactions. Our findings reveal that users’ choices when interacting with novel systems are informed by their expectations and biases, that embodied learning is built iteratively on layered multi-sensory experiences, and that there is a desire for familiarity and understanding when interacting with AI systems. We found that the materiality of our textile human-AI interfaces influenced how users choose to interact, and that users sought clarity in the AI’s role in collaborative creation. This work contributes to our understanding of how entanglement, embodiment, and materiality impact our relationships in human-AI collaborations.",Sandy Ma (ANU)*; Charles Patrick Martin (Australian National University),sandy.ma@anu.edu.au; charles.martin@anu.edu.au,sandy.ma@anu.edu.au,paper,Sandy Ma; Charles Patrick Martin,"Novel controllers, interfaces or instruments for musical expression",User studies and evaluations of NIMEs,long,oral,in person,15,,papers-5,Machine Learning and Co-Creativity,nime2025_131.pdf,,,131.jpg,,"human-AI interaction, e-textiles, tangible and embodied interaction",
160,Adaptation and Perceived Creative Autonomy in Gesture-Controlled Interactive Music,"With the variety and rapid pace of developments in Artificial Intelligence (AI), musicians can face difficulty when working with AI-based interfaces for musical expression as understanding and adaptation to AI behaviors takes time. In this paper, we explore the use of AI in an interactive music system designed to adapt to users as they learn to perform with it. We present GestAlt, an AI-based interactive music system that collaborates with a performer by analyzing their gestures and motion to generate audio changes. It uses computer vision, online machine learning, and reinforcement learning to adapt to a user's hand motion patterns and allow a user to communicate their musical goals to the system. It communicates its decision-making to the user through visualizations and its musical output. We conducted a study in which five musicians performed using this software over multiple sessions. Participants discussed how their preferences for the system’s behavior were influenced by their experiences as musicians, how adaptive reinforcement learning affected their expectations for the system’s autonomy, and how their perceptions of the system as a creatively autonomous, collaborative partner evolved as they learned how to perform with the system.",Jason Smith (Northwestern University)*; Jason Freeman (Georgia Institute of Technology),jason.smith1@northwestern.edu; jason.freeman@gatech.edu,jason.smith1@northwestern.edu,paper,Jason Smith; Jason Freeman,Music-related human-computer interaction,Gesture to sound mapping; Machine learning and artificial intelligence in NIMEs ; Machine learning in musical performance,long,oral,in person,15,,papers-5,Machine Learning and Co-Creativity,nime2025_160.pdf,,,160.jpg,,"Interactive Music, Artificial Intelligence, Machine Learning, Human-Computer Interaction",
222,Gesture-Driven DDSP Synthesis for Digitizing the Chinese Erhu,"This paper presents a gesture-controlled digital Erhu system that merges traditional Chinese instrumental techniques with contemporary machine learning and interactive technologies. By leveraging the Erhu’s expressive techniques, we develop a dual-hand spatial interaction framework using real-time gesture tracking. Hand movement data is mapped to sound synthesis parameters to control pitch, timbre, and dynamics, while a differentiable digital signal processing (DDSP) model, trained on a custom Erhu dataset, transforms basic waveforms into authentic timbre which remians sincere to  the instrument’s nuanced articulations. The system bridges traditional musical aesthetics with digital interactivity, emulating Erhu bowing dynamics and expressive techniques through embodied interaction. The study contributes a novel framework for digitizing Erhu performance practices, explores methods to align culturally informed gestures with DDSP-based synthesis, and offers insights into preserving traditional instruments within digital music interfaces.","Wenqi WU (Computational Media and Art, The Hong Kong University of Science and Technology (Guangzhou))*; Hanyu QU (Computational Media and Art, The Hong Kong University of Science and Technology (Guangzhou))",wwu252@connect.hkust-gz.edu.cn; hqu817@connect.hkust-gz.edu.cn,wwu252@connect.hkust-gz.edu.cn,paper,Wenqi WU; Hanyu QU,"Explorations of relationships between motion, gesture and music","Discussions about the artistic, cultural, and social impact of NIME technology ; Gesture to sound mapping; Machine learning in musical performance",medium,oral,in person,10,,papers-5,Machine Learning and Co-Creativity,nime2025_222.pdf,,,222.jpg,,"DDSP,Erhu,Gesture,Chinese Instrument,Interactive Music Performance",
230,Enabling Embodied Music-Making for Non-Musicians,"We present a Research through Design exploration of the potential for using tangible and embodied interactions to enable active music experiences - musicking - for non-musicians. We present the Tubularium prototype, which aims to facilitate music-making to non-musicians by not requiring any initial skill while still eliciting agency and overall, providing a meaningful experience. We present the design of the prototype and the features implemented and reflect on insights from a public event in which the prototype was trialed.",Lucía Montesínos (IT University of Copenhagen); Halfdan Hauch Jensen (IT University of Copenhagen); Anders Løvlie (IT University of Copenhagen)*,lumo@itu.dk; halj@itu.dk; asun@itu.dk,asun@itu.dk,paper,Lucía Montesínos; Halfdan Hauch Jensen; Anders Løvlie,"Novel controllers, interfaces or instruments for musical expression",Machine learning and artificial intelligence in NIMEs ; Technologies or systems for collaborative music-making,medium,oral,in person,10,,papers-5,Machine Learning and Co-Creativity,nime2025_230.pdf,,,230.jpg,,"Embodied Interaction, Tangible Interaction, Musicking, Non-Musicians, Research through Design",
187,Designing Sensory NIME for Autism,"This paper explores how sensory NIME design principles may inform the design of musical interfaces tailored for children with Autism Spectrum Disorder (ASD), focusing on their sensory processing challenges. Given the prevalence of sensory over-responsivity (SOR) and under-responsivity (SUR) in ASD, traditional sensory interventions often fail to accommodate the highly individualized and fluctuating sensory needs of autistic individuals. The authors highlight the potential for multisensory NIME to address the diverse range of sensory needs, promoting emotional regulation and sensory balance through new creative musical opportunities and activities. This paper presents research in the form of a narrative review and comparative case study of recent NIME and sensory intervention research, exploring emerging approaches, rhythm-based interventions, generative algorithms, play-centered designs and other possibilities for enhancing sensory engagement and emotional regulation. Drawing on insights from 30 recent NIME papers, this research explores the boundaries of current approaches and seeks to establish an understanding of multisensory NIME for ASD. The research underscores the profound variability in sensory profiles for ASD, necessitating a shift from clinician-directed interventions to creative, inclusive, multisensory solutions. Finally, a set of sensory NIME design principles are offered, emphasizing the importance of sensory perception, sensory equilibrium and the promotion of emotional regulation for ASD.",Aditya Arora (Swinburne University of Technology); Erica Tandori (Monash University); James Marshall (Swinburne University of Technology); Stuart Favilla (Swinburne University of Technology)*,104206190@student.swin.edu.au; erica.tandori@monash.edu; jgmarshall@swin.edu.au; sfavilla@swin.edu.au,sfavilla@swin.edu.au,paper,Aditya Arora; Erica Tandori; James Marshall; Stuart Favilla,Accessibility and interfaces for musical expression for people with special needs,"Entangled NIME: Intertwined, multilayer contexts in NIME research",medium,oral,in person,10,,papers-6,Accessibility,nime2025_187.pdf,,,187.jpg,,"music interfaces, autism, multisensory",
253,The Sound Tree Project: Developing Personal and Collective Expression with Accessible Digital Musical Instruments,"The Sound Tree Project investigates how accessible digital musical instruments (ADMIs) can champion both personal and collective musical expression. Through a sustained six-month ethnographic engagement with five performers and two support artists, we explored how to create personalised instruments for a public per- formance outcome. The technical framework combined multiple wireless motion sensor devices placed inside different objects and the development of a real-time movement-to-sound pro- cessing hub within a live coding environment. The performance was centred on an accessible sound sculpture, the Sound Tree, where digital instruments coexisted with traditional sound mak- ing objects. Drawing from our shared process of experimentation, improvisation, and personalised instrument creation, we present some key ‘magic moments’ that were woven into the final perfor- mance and discuss how they might serve as evidence of personal expression and validation of the design process. The emergence of these moments demonstrate the value of real-time system adaptation in encouraging individual expression, the importance of sustained engagement in developing personalised instruments and having effective strategies for balancing personal and collec- tive music-making. These insights have implications in developing accessible mu- sic technology and broader approaches to designing technologies that support diverse forms of creative collaboration.","Steph OHara (SensiLab, Monash University)*; Alon Ilsar (SensiLab, Monash University)",stephen.ohara@monash.edu; alon.ilsar@monash.edu,stephen.ohara@monash.edu,paper,Steph OHara; Alon Ilsar,Accessibility and interfaces for musical expression for people with special needs,"Explorations of relationships between motion, gesture and music; Gesture to sound mapping; Musical mapping strategies",medium,oral,in person,10,,papers-6,Accessibility,nime2025_253.pdf,,,253.jpg,,"Interaction, Accessible, Co-design",
295,Negotiating Entanglements in the Composition and Curation of an Ultrasonic Art Installation,"This paper explores the entangled activities of composing and curating the sound installation 'Sonographies' at a contemporary art gallery. The installation extends our work with an ultrasonic technology that sonifies and magnifies the physical entanglement of a listener with a spatial sound field to produce rich movement-sound interaction without the use of sensors. Taking a research through practice approach, we examine the process of creating 'Sonographies' while deliberately allowing the nonhuman influences of site and technology to inform creative ideation and decision-making. We propose that an attunement to entanglement foregrounds the co-production of aesthetic qualities by the entire musical assemblage and fosters a sensitivity to fragile and changeable qualities of NIMEs, contingent on specific technical, material and social situations.",Nicole Robson (Queen Mary University of London)*; Andrew  McPherson (Imperial College London ); Nick  Bryan-Kinns (University of the Arts London ),n.s.robson@qmul.ac.uk; andrew.mcpherson@imperial.ac.uk; n.bryankinns@arts.ac.uk,n.s.robson@qmul.ac.uk,paper,Nicole Robson; Andrew  McPherson; Nick  Bryan-Kinns,"Entangled NIME: Intertwined, multilayer contexts in NIME research",Interactive sound art and sound installations ; Practice-based research approaches/methodologies/criticism,long,oral,in person,15,,papers-7,Entangled NIME,nime2025_295.pdf,,,295.jpg,,"entanglement, sound installation, ultrasound, practice",
74,"Hacking Sound, Hacking History: Patricia Cadavid and the Electronic_Khipu_","To better understand researcher and artist Patricia Cadavid Hinojosa’s instrument the Electronic\_Khipu\_, we must define the project as an instance of hacking. Cadavid deconstructs colonial understandings of the Andean device known as the khipu, pulling apart the academic view of khipus as artifacts to be deciphered, the strict delineation between administrative and ritualistic uses of the khipu, and the separation of the oral tradition from the object. Through deliberate design choices and musical expression in performance, Cadavid emphasizes the inextricability of coding, art, and ritual by creating a tactile device that re-tells history and challenges the false oppositional binary between Indigeneity and technology. Understanding this project of digital lutherie as an act of creation through hacking -- specifically as the deconstruction and reconfigurement of artistic and historical components, utilizing scholars Astrida Neimani's and Vít Bohal's definitions -- allows us to appreciate its power.",Margaret Needham (Smith College)*,mneedham@smith.edu,mneedham@smith.edu,paper,Margaret Needham,"Discussions about the artistic, cultural, and social impact of NIME technology","Entangled NIME: Intertwined, multilayer contexts in NIME research; User studies and evaluations of NIMEs",medium,oral,in person,10,,papers-8,Historical and Cultural Reflections,nime2025_74.pdf,,,74.jpg,,"Hacking,NIME,Khipu,Tangible Interface",
134,NIME: A Mis-User's Manual,"Ever since the initiating workshop at the 2001 ACM CHI'01 Conference, annual New Interfaces for Musical Expression conferences have seen a proliferation of work featuring different forms of music, research values, philosophical, ethical and political standpoints. The 2025 ‘Entangled’ theme celebrates this diversity of creative, technical, and social ‘intelligencings’ (Thrift [68, p153-154]). It is precisely the non- or pluri-paradigmatic character of NIME that is its strength. Drawing on Maria Lugones [41], we characterise NIME less as an entangled weave—where threads maintain their separate yet assembled and interconnected character—than as a ‘curdling’ where relationships are more complex, varied, mutually interrupting and shaping, indeterminate and unknown without careful dialogue. We do not consider it appropriate to offer unifying frameworks or mappings with often hidden authoritarian implications. Rather, following Rancière [5], we prefer a radically democratic dissensus and, following Lugones, a spirit of ‘festive resistance’ where we poke at the limits of our inherited metaphors to undermine attempts to provide a fixed orderliness, (re)framing topics to kickstart exchange on new fertile grounds for collaboration. Multiple kinds and collisions of agency, and the lively openness of what some might deem ‘failure’ are prioritised over the often inhibiting closure and certainty of ‘success’ [e.g. 10, 11, 12, 40]. Our topics include: multiple ways of making as a means of maximising exposure to possible failure; shifting from interfaces to interfacing to create arenas for action rather than tools for purposes; foregrounding risk, inefficiency and forgetting; formulating improvisation as knowing-when and composing-the-now; performance practice, settings and contingencies; alternative resourcings/reframings for research; a wild spirit of tactical oppositionalism, dynamic uncompromise, and existential pluralism, to embrace the independence of divergent voices.","Sally Jane Norman (New Zealand School of Music, Victoria University of Wellington)*; Paul Stapleton (SARC, Queen's University Belfast); John Bowers (Independent Artist Researcher)",sallyjane.norman@vuw.ac.nz; p.stapleton@qub.ac.uk; john.m.bowers@gmail.com,sallyjane.norman@vuw.ac.nz,paper,Sally Jane Norman; Paul Stapleton; John Bowers,"Discussions about the artistic, cultural, and social impact of NIME technology","Entangled NIME: Intertwined, multilayer contexts in NIME research; Historical, theoretical or philosophical discussions about designing or performing with new interfaces ; Practice-based research approaches/methodologies/criticism",long,oral,in person,15,,papers-8,Historical and Cultural Reflections,nime2025_134.pdf,,,134.jpg,,"curdling, performance, festive resistance, dissensus, existential pluralism",
213,O一: An Epistemic DMI for Cross-Cultural Reflection on Time and Music,"This paper introduces a Digital Musical Instrument (DMI) that inscribes a linear and a circular conception of time, inspired by Western and Eastern time philosophies. The DMI employs two 3D-printed boards equipped with ESP32 chips for wireless communication and WS2812 LEDs providing visual representation feedback, and interactive boxes, each fitted with a light sensor and ESP32 Mini boards. Such interface is designed to be coupled with a software counterpart for sound generation.The project originates from a collaboration with two composers from diverse cultural backgrounds - one Chinese and one Italian. Through collaborative design and co-composing practice, the proposed DMI emerged as an epistemic tool, promoting cultural understanding and critically highlighting the socio-cultural role of technology. Through such process, the significance of rediscovering time in contemporary globalization and philosophy was explored, challenging the conception of time as a mere measurement parameter and striving to reveal the importance of understanding the role of time across different cultural contexts. This project wishes to expand the constitutive role of musical time, demonstrating its diversity and prompting a reflective layer of the perception of performative and musical time in NIME.","Hanyu Qu (Hong Kong University of Science and Technology(Guangzhou))*; Francesco Dal Rí  (Department of Information Engineering and Computer Science University of Trento, IT); Hao Zou (University of Missouri-Kansas City); Hanqing Zhou (Southern University of Science and Technology); Raul Masu (Hong Kong University of Science and Technology (Guangzhou))",hqu817@connect.hkust-gz.edu.cn; francesco.dalri-2@unitn.it; zouhaocomposer@gmail.com; 12331483@mail.sustech.edu.cn; raul@raulmasu.org,hqu817@connect.hkust-gz.edu.cn,paper,Hanyu Qu; Francesco Dal Rí; Hao Zou; Hanqing Zhou; Raul Masu,"Discussions about the artistic, cultural, and social impact of NIME technology","Entangled NIME: Intertwined, multilayer contexts in NIME research; Technologies or systems for collaborative music-making",long,oral,in person,15,,papers-8,Historical and Cultural Reflections,nime2025_213.pdf,,,213.jpg,,"Digital Musical Instrument (DMI), Epistemic DMI, co-composition, time conception",
58,Collaborative Musical Expression Through Interactive VR Scores,"While the technical affordances of virtual reality (VR) have pro- vided new ways for artists to aestheticize immersion, spectator agency, embodiment and multi-sensory engagement, they have also opened new possibilities for composers interested in ex- ploring how interactive musical scores might become a means through which collaboration itself becomes the locus of aesthetic expression. In this paper, the author will provide an overview of an ongoing project which explores new ways of thinking about musical collaboration in VR through the 3D visualization of interactive, graphic scores adapted from works by composers Earle Brown, Christian Wolff, and Toru Takemitsu. The research demonstrates how VR can transform traditional score interpreta- tion by creating dynamic, interactive environments that enable collaborative musical expression, challenge conventional nota- tion, and offer novel ways of negotiating musical performance through networked, multi-user interactions.",David Kim-Boyle (The University of Sydney)*,david.kim-boyle@sydney.edu.au,david.kim-boyle@sydney.edu.au,paper,David Kim-Boyle,"Extended reality environments: augmented, virtual, mixed reality",Technologies or systems for collaborative music-making,long,oral,in person,15,,papers-9,Extended Reality,nime2025_58.pdf,,,58.jpg,,"Virtual Reality, Multiplayer, Networking, Collaboration",
59,SVOrk: Stanford Virtual Reality Orchestra,"This paper chronicles the creation of the Stanford Virtual Reality Orchestra (SVOrk), a new computer music ensemble where both performers and audience engage in a shared, fully immersive virtual reality (VR) chamber-esque concert experience. Motivated to explore group-based live performance within VR, SVOrk has designed and crafted virtual musical interfaces, fantastical 3D-modeled environments, and a network infrastructure to support real-time shared participation. Inherent within this initiative is a reimagining of conventional concert experiences, introducing virtual lobbies, customizable avatars, and immersive audience interactions. These experimental features explore new forms of social presence, audience identities, and expressive communication to help address the overarching question, “What does it mean to participate in a VR musical performance?” SVOrk’s premiere concert took place in June 2024 with five performers and approximately 60 audience members (across five sessions), featuring a program of six musical works. This paper describes the motivations behind SVOrk, its research and development process—including designs for networking, avatar, and audience interaction—and takeaways from the premiere concert. We also present audience feedback and reflect on our experiences in curating group VR performances.","Kunwoo Kim (Center for Computer Research in Music and Acoustics, Stanford University)*; Andrew Zhu (Center for Computer Research in Music and Acoustics, Stanford University); Eito Murakami (Center for Computer Research in Music and Acoustics, Stanford University); Marise van Zyl (Center for Computer Research in Music and Acoustics, Stanford University); Yikai Li (Center for Computer Research in Music and Acoustics, Stanford University); Max Jardetzky (Center for Computer Research in Music and Acoustics, Stanford University); Ge Wang (Center for Computer Research in Music and Acoustics, Stanford University)",kunwoo@ccrma.stanford.edu; azaday@ccrma.stanford.edu; eitom@ccrma.stanford.edu; marise@ccrma.stanford.edu; yikaili@ccrma.stanford.edu; mjardetz@ccrma.stanford.edu; ge@ccrma.stanford.edu,kunwoo@ccrma.stanford.edu,paper,Kunwoo Kim; Andrew Zhu; Eito Murakami; Marise van Zyl; Yikai Li; Max Jardetzky; Ge Wang,"Extended reality environments: augmented, virtual, mixed reality",New music performance paradigms,long,oral,in person,15,,papers-9,Extended Reality,nime2025_59.pdf,,,59.jpg,,"Virtual Reality, VR Concert, VR Orchestra, VR Instrument Design",
266,Physical Music Albums in the Digital Era: Exploring Experiential Value Through the Integration of AR,"This study explores physical music albums in the digital age, as well as the creation of new music experiences through the integration of Augmented Reality (AR) into physical albums. An online survey was conducted to examine the differences in experiences between digital and physical albums, and this informed the development of a physical music album incorporating AR. We provided 8 K-POP fans, who engage with physical albums more frequently than fans of other genres, the opportunity to test existing AR-integrated albums and a new prototype featuring AR packaging animations, multiplayer virtual concerts, and interactive photo features. The results underscored the importance of understanding and respecting fan culture when using AR. The results suggest that, compared to digital albums, physical albums derive significant experiential value from traditional supplementary materials such as booklets and lyric cards. However, AR has the potential as a complementary new material that provides users with novel experiences.  This work leads to a reconsideration of Walter Benjamin’s concept of “aura,” which critiques the reproducibility of art.",Ena Fumihira (University of Technology Sydney)*; Andrew Johnston (University of Technology Sydney),ena.svt@gmail.com; andrew.johnston@uts.edu.au,ena.svt@gmail.com,paper,Ena Fumihira; Andrew Johnston,"Extended reality environments: augmented, virtual, mixed reality","Discussions about the artistic, cultural, and social impact of NIME technology ; Evaluation and user studies of new interfaces for musical expression ; Music-related human-computer interaction",medium,oral,in person,10,,papers-9,Extended Reality,nime2025_266.pdf,,,266.jpg,,"Augmented Reality (AR), Physical Albums, Music Experience, Fan Culture, Walter Benjamin",
277,Exploring the impact of spatial awareness on large-scale AR DMIs,"Large-scale Digital Musical Instruments (DMIs) offer immersive performance experiences and rich forms of expression, but often pose physical challenges and limit accessibility. Traditional large-scale DMIs' size limits performers' ability to interact with the instrument, causing discomfort when engaging with distant components, highlighting the need for more flexible and user-friendly large-scale DMI designs. We present an Augmented Reality (AR) DMI that removes physical constraints by allowing performers to customise the instrument’s size and layout according to their performance environment. We aim to show how AR-based configuration supports immersive performance, promotes expressive gestures, and improves spatial awareness without sacrificing large-scale instrument capabilities. Our user study revealed increased physical engagement and spatial immersion, a strong sense of ownership, and a positive user experience. These findings indicate that our AR DMI is creatively empowering, reasonably addressing the constraints of large-scale instruments. Our research emphasises the potential of AR to enable flexible and customisable DMI design where interfacems can be adapted to suit the neeeds of individual performers.",Qiance Zhou (Australian National University)*; Charles Patrick Martin (Australian National University),u7520051@anu.edu.au; charles.martin@anu.edu.au,u7520051@anu.edu.au,paper,Qiance Zhou; Charles Patrick Martin,"Extended reality environments: augmented, virtual, mixed reality",Music-related human-computer interaction ; User studies and evaluations of NIMEs,medium,oral,in person,10,,papers-9,Extended Reality,nime2025_277.pdf,,,277.jpg,,"augmented reality, scale, user study",
20,XR Musical Keyboard: An Extended Reality Keyboard with an Arbitrary Number of Keys and Pitches,"We introduce the Extended Reality (XR) Musical Keyboard, a system allowing users to overlay a virtual keyboard onto a tabletop surface, such as a standard PC keyboard. This virtual keyboard is highly customizable: users can freely program the number of keys and their respective pitches. Modern software instruments offer advanced capabilities, including microtonal scales (pitches outside the standard 12-tone equal temperament). However, playing these instruments often remains challenging due to the lack of corresponding physical hardware. Our proposed solution addresses this gap by projecting a programmable virtual keyboard onto a tangible object within the XR space. This approach combines the software's flexibility with the tactile feedback of a physical surface, enhancing playability. Users can simplify the keyboard layout (e.g., fewer keys than a piano) or expand it beyond conventional limits to explore new expressive possibilities, particularly for microtonal music. We conducted a small pilot study (N=4) involving participants mostly inexperienced with keyboards to gather preliminary feedback on the interface's ease of use for performance.",Tatsunori Hirai (Komazawa University)*; Jack Topliss (University of Cantebury); Thammathip Piumsomboon (University of Cantebury),thirai@komazawa-u.ac.jp; Jack.a.m.topliss@gmail.com; tham.piumsomboon@canterbury.ac.nz,thirai@komazawa-u.ac.jp,paper,Tatsunori Hirai; Jack Topliss; Thammathip Piumsomboon,"Extended reality environments: augmented, virtual, mixed reality",Music-related human-computer interaction,medium,poster,in person,5,,posters-1,,nime2025_20.pdf,,,20.jpg,,"XRMI, Microtones, Microtonal keyboard",
31,pybela: a Python library to interface scientific and physical computing,"Workflows to obtain, examine and prototype with sensor data often involve a back and forth between environments, platforms and programming languages. Usually, sensors are connected to physical computing platforms, and solutions to transmit data to the computer often rely on low-bandwidth communicating channels. It is not obvious how to interface physical computing platforms with data science environments, which also operate under distinct constraints and programming styles. We introduce pybela, a Python library that facilitates real-time, high-bandwidth, bidirectional data streaming between the Bela embedded computing platform and Python, bridging the gap between physical computing environments and data-driven workflows. In this paper, we outline its design, implementation and applications, including deep learning examples.",Teresa Pelinski (Queen Mary University of London)*; Giulio Moro (Augmented Instruments Ltd); Andrew McPherson (Imperial College London),t.pelinskiramos@qmul.ac.uk; giulio@bela.io; andrew.mcpherson@imperial.ac.uk,t.pelinskiramos@qmul.ac.uk,paper,Teresa Pelinski; Giulio Moro; Andrew McPherson,"Software frameworks, interface protocols, and data formats, for supporting musical interaction",Machine learning and artificial intelligence in NIMEs ; Machine learning in musical performance,long,poster,in person,5,,posters-1,,nime2025_31.pdf,,,31.jpg,,"prototyping, physical computing, machine learning, embedded AI",
33,Waveform Autoencoding at the Edge of Perceivable Latency,"We introduce an audio plugin implementation of BRAVE, a waveform autoencoder presented recently, that affords Neural Audio Synthesis with low latency and jitter. As a redesign of the well-known RAVE model, BRAVE introduces a series of architectural modifications for supporting instrumental interaction with almost imperceptible latency (<10 ms) and jitter (~ 3 ms). By comparing both designs, we highlight key architectural differences between the models that impact their instrumental performance capability, arguing that no model fits all purposes, and calling for their careful selection for each interactive design. Finally, we discuss challenges and opportunities for leveraging low-latency waveform autoencoders to develop interactive systems, such as Digital Musical Instruments, that can foster control intimacy through enhanced responsiveness and space for nuance.",Franco Caspe (Queen Mary University of London - Centre For Digital Music)*; Andrew McPherson (Imperial College London); Mark Sandler (	Queen Mary University of London - Centre For Digital Music),f.s.caspe@qmul.ac.uk; andrew.mcpherson@imperial.ac.uk; mark.sandler@qmul.ac.uk,f.s.caspe@qmul.ac.uk,paper,Franco Caspe; Andrew McPherson; Mark Sandler,Performance rendering and generative algorithms,"New music performance paradigms; Novel controllers, interfaces or instruments for musical expression",short,poster,in person,5,,posters-1,,nime2025_33.pdf,,,33.jpg,,"Instrumental Performance, Timbre Transfer, Low Latency, Neural Audio Synthesis",
49,Introducing EG-IPT and ipt~: a novel electric guitar dataset and a new Max/MSP object for real-time classification of instrumental playing techniques,"This paper presents two key contributions to the real-time classification of Instrumental Playing Techniques (IPTs) in the context of NIME and human-machine interactive systems: the EG-IPT dataset and the ipt~ Max/MSP object. The EG-IPT dataset, specifically designed for electric guitar, encompasses a broad range of IPTs captured across six distinct audio sources (five microphones and one direct input) and three pickup configurations. This diversity in recording conditions provides a robust foundation for training accurate models. We evaluate the dataset by employing a Convolutional Neural Network-based classifier (CNN), achieving state-of-the-art performance across a wide array of IPT classes, thereby validating the dataset’s efficacy. The ipt~ object is a new Max/MSP external enabling real-time classification of IPTs via pre-trained CNN models. While in this paper it's demonstrated with the EG-IPT dataset, the ipt~ object is adaptable to models trained on various instruments. By integrating EG-IPT and ipt~, we introduce a novel, end-to-end workflow that spans from data collection, model training to real-time classification and human-computer interaction. This workflow exemplifies the entanglement of diverse components (data acquisition, machine learning, real-time processing, and interactive control) within a unified system, advancing the potential for dynamic, real-time music performance and human-computer interaction in the context of NIME.","Marco Fiorini (IRCAM, Sorbonne Université, CNRS)*; Nicolas Brochec (Tokyo University of the Arts); Joakim Borg (IRCAM); Riccardo Pasini (UNIFE Università di Ferrara)",marco.fiorini@ircam.fr; nicolas.brochec@pm.me; joakim.borg@ircam.fr; riccardo01.pasini@edu.unife.it,marco.fiorini@ircam.fr,paper,Marco Fiorini; Nicolas Brochec; Joakim Borg; Riccardo Pasini,Machine learning and artificial intelligence in NIMEs,"Entangled NIME: Intertwined, multilayer contexts in NIME research; Music-related human-computer interaction ; New music performance paradigms",long,poster,in person,5,Wants to borrow electric guitar for the poster session.,posters-1,,nime2025_49.pdf,,,49.jpg,,"Instrumental Playing Techniques, Electric Guitar, Music Classification, Real-Time, Music AI, Python, Max/MSP, NIME",
62,You’re An Instrument!: Creating active music-making experiences through worldbuilding and storytelling,"This paper outlines the development and key discoveries relating to active audience participation within You’re an Instrument!, an immersive childrens’ theatre show that turns a planted audience member into a musical instrument. We outline the use of wireless gestural instruments in the show, exploring their novel use as hidden props and theatrical devices that help invite audience members into a fictional world. Through the creation of this fictional world, the audience members became more actively involved in music making with these devices in the last third of the show. This paper is a call for instrument designers to consider using worldbuilding and storytelling techniques to more actively engage audience members in discovering the workings of new instruments.","Ciaran Frame (SensiLab, Monash University)*; Erick Mitsak (SensiLab, Monash University); Alon Ilsar (SensiLab, Monash University)",ciaran.frame@monash.edu; erick.mitsak@monash.edu; alon.ilsar@monash.edu,ciaran.frame@monash.edu,paper,Ciaran Frame; Erick Mitsak; Alon Ilsar,Accessibility and interfaces for musical expression for people with special needs,"Gesture to sound mapping; Musical mapping strategies ; Novel controllers, interfaces or instruments for musical expression",long,poster,in person,5,,posters-1,,nime2025_62.pdf,,,62.jpg,,"worldbuilding, instrument deign, accessibility",
64,Sculpting the Sound Atom: Towards Per-Grain Parameterisation and Interfacing in Granular Synthesiser Design,"This paper presents a number of custom-designed granular synthesisers built around interfacing with sound grains on an ‘atomic’ level. Developed in Max/MSP by the first author (Nathan Carter), these synthesisers explore per-grain voice parameterisation that uniquely interfaces with individual grain signal processing properties in larger granular sequences. The paper outlines how these synthesisers provided the sound materials to compose Carter’s original soundscape work ‘Matter and Void’ – conceptually in- spired by ancient Epicurean physics and painterly expositions on atomism in Lucretius’ poem ‘The Nature of Things’ (c. 55 BC).",Nathan Carter (Victoria University of Wellington - Te Kōkī New Zealand School of Music)*; Jim Murphy (Victoria University of Wellington - Te Kōkī New Zealand School of Music	); Mo Zareei (Victoria University of Wellington - Te Kōkī New Zealand School of Music	),carternath1@myvuw.ac.nz; jim.murphy@vuw.ac.nz; mo.zareei@vuw.ac.nz,carternath1@myvuw.ac.nz,paper,Nathan Carter; Jim Murphy; Mo Zareei,"Novel controllers, interfaces or instruments for musical expression",Practice-based research approaches/methodologies/criticism,short,poster,in person,5,,posters-1,,nime2025_64.pdf,,,64.jpg,,"Granular Synthesiser, Per-Grain Parameterisation, Atomism, Max/MSP, Practice-Based Research",
66,Harmonix Series: Accessible Digital Musical Instruments for Mindfulness and Creativity," This paper introduces the Harmonix series, a collection of Accessible Digital Musical Instruments (ADMIs) designed to enhance mood stability and mindfulness through intuitive and interactive music-making. Recognising the barriers posed by traditional digital musical instruments, including steep learning curves, high costs, and uninspiring outputs, Harmonix prioritises affordability, portability, and user-friendly interfaces to cater to individuals with no prior musical training. The study evaluates two instruments, ZenithChimes and Equilibrio. ZenithChimes employs touch-sensitive keys mapped to meditative tones in the Aeolian mode, promoting creativity and relaxation. Equilibrio, a 3D-printed ""stone stack,"" uses tilt gestures to modulate soundscapes, symbolising balance and harmony. Both instruments integrate calming auditory outputs and minimalist design aesthetics to create an engaging and meditative experience. A workshop-based study with 10 participants, spanning diverse backgrounds, demonstrated the instruments' accessibility and therapeutic potential. Results showed that participants found the instruments easy to use, aesthetically appealing, and suitable for mindfulness practices, with 70 percent identifying their integration into meditation or yoga sessions as beneficial. However, feedback highlighted the need for more customisation options, particularly in Equilibrio's soundscapes. By bridging art, technology, and mindfulness, Harmonix fosters creative exploration and emotional regulation, with implications for therapeutic, educational, and artistic applications. Future work will explore sustainability, inclusivity, and multi-sensory feedback to enhance the instruments’ design and impact. This study underscores the potential of ADMIs to transcend conventional music-making, offering innovative tools for well-being and self-expression.","Wing Hei Cheryl Hui (Individual Researcher)*; Patrick Hartono (Goldsmiths, University of London)",wingheicherylhui@gmail.com; Patrick.Hartono@gold.ac.uk,wingheicherylhui@gmail.com,paper,Wing Hei Cheryl Hui; Patrick Hartono,Accessibility and interfaces for musical expression for people with special needs,"Interactive sound art and sound installations ; Novel controllers, interfaces or instruments for musical expression",medium,poster,in person,5,,posters-1,,nime2025_66.pdf,,,66.jpg,,"Accessible Digital Musical Instruments (ADMIs), Music and Mindfulness, HCI, Tactile interfaces for music expression",
70,Creating a White Noise Instrument for Collaborative Improvisation,"This paper introduces shiki, a virtual instrument developed for performing Renga for White Noise, an interdisciplinary project that transmediates Japanese renga poetry principles into a framework for collaborative improvisation with human and AI agents. We discuss two areas: (1) our transmediation of renga’s structuring principles into shiki’s design (2) the technical aspects behind the AI agent’s performance with shiki. Through its interdisciplinary and intercultural entanglements with renga, transmediation as a method can illuminate new perspectives on the design and performance of NIMEs.",Austin Oting Har (University of the Virgin Islands)*; Kurt Mikolajczyk (University of New South Wales),austinotinghar@gmail.com; k.mikolajczyk@unsw.edu.au,austinotinghar@gmail.com,paper,Austin Oting Har; Kurt Mikolajczyk,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Novel controllers, interfaces or instruments for musical expression ; Performance rendering and generative algorithms ; Technologies or systems for collaborative music-making",short,poster,in person,5,,posters-1,,nime2025_70.pdf,,,70.jpg,,"Instrument design, Japanese renga poetry, white noise, collaborative improvisation, evolutionary algorithms",
71,Collaboration and Recursion: reflections on Chinese calligraphy and feedback,"This paper delivers reflections on collaboration between a sound artist and an artist specializing in Chinese calligraphy, that resulted in the creation of a sound installation combining contemporary Chinese calligraphy with electroacoustic feedback. Adopting the “reflection-on-action” approach, the authors engaged in an in-depth discussion, revisiting the details of the creative process based on the extensive documentation arranged in a form of visual diary. The paper highlights three orders of recursivity that are either physically present in the work (feedback), defined the creative process (collaboration) and used as analytical tools (ecology) to discuss the dynamics of collaboration and cultural influences in NIME practice.",Iurii Kuzmin (The Hong Kong University of Science and Technology)*; Raul Masu (The Hong Kong Unversity of Science and Technology (Guangzhou)); Omar Al Kanawati (China Academy of Art ),ikuzmin@connect.ust.hk; raul@raulmasu.org; omarkanawati@post.sk,ikuzmin@connect.ust.hk,paper,Iurii Kuzmin; Raul Masu; Omar Al Kanawati,Practice-based research approaches/methodologies/criticism,"Discussions about the artistic, cultural, and social impact of NIME technology ; Interactive sound art and sound installations",long,poster,in person,5,,posters-1,,nime2025_71.pdf,,,71.jpg,,"Chinese calligraphy, collaboration, feedback, sound installation",
94,A Computer Application to Explore 53-Tone Equal Temperament Harmonies Through Modal Interchange,"We present a novel computer application for real-time exploration of microtonal harmonies through intuitive visualization and integrated MIDI controllers, bridging theoretical concepts and practical musical applications. We extend modern harmonic principles from 12-tone equal temperament (12-TET), incorporating interval distinctions from 31-TET (subminor, neutral, supermajor), and further expanding into detailed harmonic possibilities of 53-tone equal temperament (53-TET). Our application leverages modal interchange and parallel chord substitutions, offering intuitive navigation through microtonal harmonic trajectories. The implementation utilizes MIDI Polyphonic Expression (MPE) via our custom MaxForLive application, The Bridge, ensuring precise microtonal control and compatibility with digital audio workstations (Ableton Live 11/12). The system includes real-time visualization, interactive chord manipulation, and a comprehensive Scale Editor for harmonic experimentation. Through practical examples and theoretical analysis, we demonstrate how this approach reveals new harmonic possibilities while maintaining connections to established modal frameworks. This research contributes to the growing microtonal music field by providing theoretical foundations and practical tools that incorporate extended tuning systems into contemporary musical practice.",David Dalmazzo (KTH)*; Ken Déguernel ( UMR 9189 – CRIStAL ); Bob Sturm (KTH),dalmazzo@kth.se; ken.deguernel@univ-lille.fr; bobs@kth.se,dalmazzo@kth.se,paper,David Dalmazzo; Ken Déguernel; Bob Sturm,Interface protocols and data formats supporting musical interaction,Musical mapping strategies ; Music-related human-computer interaction,medium,poster,remote,5,,posters-1,,nime2025_94.pdf,,,94.jpg,,"Microtonality, Music Theory, Modal Interchange, Harmony, Navigation, 53 Tone Equal Temperament",
123,Making the Immaterial Material: A Diffractive Approach Toward a Politics of Material Culture Within NIME,"Traditional Human-Computer Interaction has often been critiqued for its ostensibly opaque position on ethical, ontological, and epistemological concerns, particularly in relation to completed design artifacts. More recently, similar criticisms have been directed at the New Interfaces for Musical Expression (NIME) community for its relative silence on contemporary political issues. However, it is possible that an implicit ethics of material culture is already embedded within NIME discourse — one that could be critically examined and potentially mobilized as a foundation for a more explicitly political ethics. Inspired by feminist discourse, namely Karen Barad's theory of agential realism, and contextualized through Bruno Latour's remarks regarding the ethics of design, this paper explores the possibilities of entanglement in DMI design. We begin with a discussion of diffraction and entanglement followed by a brief overview of values-oriented and ""world-building"" theoretical models and methodologies of design research. We continue with our generative ""DMI-as-apparatus"" approach to diffractive methodology and conclude with a case study BRAIDS_, a digital music instrument based upon the Black American cultural practice of hair braiding, that examines critical design decisions that are otherwise deemed invisible by traditional methods of scientific inquiry.",Brittney Allen (Imperial College London)*; Andrew Mcpherson (Imperial College London); Alexandria Smith (Georgia institute of Technology); Jason Freeman (Georgia Institute of Technology),brittneyjuliet@gmail.com; andrew.mcpherson@imperial.ac.uk; alexandria.smith@gatech.edu; jason.freeman@gatech.edu,brittneyjuliet@gmail.com,paper,Brittney Allen; Andrew Mcpherson; Alexandria Smith; Jason Freeman,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Historical, theoretical or philosophical discussions about designing or performing with new interfaces",medium,poster,in person,5,,posters-1,,nime2025_123.pdf,,,123.jpg,,"Entanglement, Diffraction, Assemblification",
163,Towards the Continuous Harmonium: Replicating the Continuous Keyboard,"In our effort to develop an augmented harmonium to enable the performance of continuous pitch ornamentation while preserving typical harmonium gestures, we have replicated the continuous keyboard presented by McPherson et al. in prior work. We present 1) our adaptations to the design of the sensing system, 2) our preliminary novel mapping design, and 3) a report on our replication process.",Travis West (McGill University); Ninad Puranik (McGill University )*; Marcelo Wanderley (McGill University); Gary Scavone (McGill University),travis.west@mail.mcgill.ca; ninad.puranik@mail.mcgill.ca; marcelo.wanderley@mcgill.ca; gary.scavone@mcgill.ca,ninad.puranik@mail.mcgill.ca,paper,Travis West; Ninad Puranik; Marcelo Wanderley; Gary Scavone,"Augmented, embedded and hyper instruments","Gesture to sound mapping; Musical mapping strategies ; Novel controllers, interfaces or instruments for musical expression",medium,poster,in person,5,,posters-2,,nime2025_163.pdf,,,163.jpg,,"free-reed instruments, harmonium, Hindustani music, keyboard interfaces",
124,The Sparksichord: Practical Implementation of a Lorentz Force Electromagnetic Actuation and Feedback System,"In line with a sustained community interest in electromagnetic actuation of musical instruments, we describe practical considerations for Lorentz Force actuation in conductive strings, exemplified by the Sparksichord – an augmented harpsichord that uses Lorentz Force actuation, optical feedback, and analog circuitry to sustain vibrations of its brass strings. Electromagnetically-actuated and feedback instruments have grown increasingly popular in NIME, though most systems rely on the use of solenoid-style electromagnetic coils. By running current through the string itself, Lorentz Force actuation offers an alternate arrangement of magnets and wire that can afford new modes of interaction, a broader frequency response, and cheaper implementation.  We aim to empower practitioners with a toolbox for designing and building actuated instruments of this style and describe our specific implementation for this instrument.",Adam Schmidt (Imperial College London)*; Jeffrey Snyder (Princeton University); Gian Torrano Jacobs (Montclair State University); Joyce Chen (Princeton University); Joseph Gascho (University of Michigan); Andrew McPherson (Imperial College London),a.schmidt24@imperial.ac.uk; josnyder@princeton.edu; torranojacog1@montclair.edu; joyce.chen@princeton.edu; jgascho@umich.edu; andrew.mcpherson@imperial.ac.uk,a.schmidt24@imperial.ac.uk,paper,Adam Schmidt; Jeffrey Snyder; Gian Torrano Jacobs; Joyce Chen; Joseph Gascho; Andrew McPherson,"Sensor and actuator technologies, including haptics and force feedback devices","Augmented, embedded and hyper instruments",long,poster,in person,5,,posters-2,,nime2025_124.pdf,,,124.jpg,,"Electromagnetic actuation, sustain, augmented instruments, analog circuits, optical sensing",
135,The Shadow Harvester: Sonifying the Body Through Light,"This paper explores the use of a violinist’s shadow as input for a NIME prototype called the Shadow Harvester. For centuries, shadows have captivated humanity’s imagination, and this project follows many artists, philosophers, and researchers equally captivated by the potential of shadows and silhouettes. This interface consists of a semi-translucent screen embedded with light-detecting sensors. These sensors register the movement of the violinist’s shadow and produce data that can be mapped to generate, trigger, or process sound in Max/MSP. The Shadow Harvester turns a human shadow into a real-time, life-size avatar, splitting the attention of the violinist between their shadow self and carnal self. They are ensnared in a web of sensors that require the same attention as the visceral joints in their body because any movement carries sonic repercussions through either their physical body playing the violin or their shadow body “playing” this interface. The Shadow Harvester creates a highly entangled feedback loop between the violinist, centuries of violin performance practice, and composition. As such, it carries the potential to encourage new ways of incorporating movement into the folds composition, notation, and performance.",Darlene Castro (University of Chicago)*,dmcastro@uchicago.edu,dmcastro@uchicago.edu,paper,Darlene Castro,"Novel controllers, interfaces or instruments for musical expression","Entangled NIME: Intertwined, multilayer contexts in NIME research; Explorations of relationships between motion, gesture and music; Musical mapping strategies",medium,poster,in person,5,,posters-2,,nime2025_135.pdf,,,135.jpg,,"NIME, Augmented Instruments, Performer-Technology Interaction, Light Sensors",
139,Exploring Musical Creation Through Brain-Body Digital Musical Instruments,"Brain-Body Digital Musical Instruments (BBDMI) merge physiological signals with real-time sound processing, enabling performers to use Electromyographic (EMG) data for musical expression. This study explores the creative and technical potential of BBDMI, focusing on signal acquisition, calibration, and mapping for use in composition and performance. Demonstrations with flute, piano showcase its ability to enhance expressivity through gestural control. While key advancements include improved signal stability and refined mapping, challenges such as connectivity issues and notation limitations remain. This research highlights BBDMI’s promise as a transformative tool in contemporary music.",Yue Wang (CIRMMT Student member); Meiling Wu (CIRMMT Student member)*,yue.wang.2@umontreal.ca; meiling.wu@mail.mcgill.ca,meiling.wu@mail.mcgill.ca,paper,Yue Wang; Meiling Wu,Gesture to sound mapping,Evaluation and user studies of new interfaces for musical expression,medium,poster,in person,5,,posters-2,,nime2025_139.pdf,,,139.jpg,,"BBDMI, Gesture control, Composition",
140,Mixer Metaphors: audio interfaces for non-musical applications,"The NIME conference traditionally focuses on interfaces for music and musical expression. In this paper we reverse this tradition to ask, can interfaces developed for music be successfully appropriated to non-musical applications? To help answer this question we designed and developed a new device, which uses interface metaphors borrowed from analogue synthesisers and audio mixing to physically control the intangible aspects of a Large Language Model. We compared two versions of the device, with and without the audio-inspired augmentations, with a group of artists who used each version over a one week period. Our results show that the use of audio-like controls afforded more immediate, direct and embodied control over the LLM, allowing users to creatively experiment and play with the device over it's non-mixer counterpart. Our project demonstrates how cross-sensory metaphors can support creative thinking and embodied practice when designing new technological interfaces.",Tace McNamara (Monash University); Jon McCormack (Monash University)*; Maria Teresa Llano (University of Sussex),Tace.McNamara@monash.edu; jon.mccormack@monash.edu; m.t.llano-rodriguez@sussex.ac.uk,jon.mccormack@monash.edu,paper,Tace McNamara; Jon McCormack; Maria Teresa Llano,Music-related human-computer interaction,Machine learning and artificial intelligence in NIMEs ; Musical mapping strategies ; Practice-based research approaches/methodologies/criticism,long,poster,in person,5,,posters-2,,nime2025_140.pdf,,,140.jpg,,"audio mixing, creativity, interface design, LLM",
148,Tiny Touch Instruments: Composing for Collaborative Mobile Performance,"This paper explores Tiny Touch Instruments (TTIs), a set of mobile instruments, and their role in facilitating collaborative, unrehearsed music-making. Through the composition and performance of two pieces, Skating and Skipping, this work investigates how multimodal notation and instrument design can shape performer experience, interaction, and engagement. Performances were documented through participant observation, interviews, and a survey, revealing key themes such as the role of notation in guiding improvisation, the balance between agency and unpredictability in digital instruments, and the recontextualization of mobile devices as musical tools.",Rebecca Abraham (Dartmouth College)*,rebecca.s.abraham.gr@dartmouth.edu,rebecca.s.abraham.gr@dartmouth.edu,paper,Rebecca Abraham,New performance paradigms for mobile music-making,Musical mapping strategies ; Technologies or systems for collaborative music-making ; User studies and evaluations of NIMEs,medium,poster,in person,5,,posters-2,,nime2025_148.pdf,,,148.jpg,,"mobile music-making,collaboration,touch instruments",
156,Live Improvisation with Fine-Tuned Generative AI: A Musical Metacreation Approach,"This paper presents a pipeline to integrate a fine-tuned open-source text-to-audio latent diffusion model into a workflow with Ableton Live for the improvisation of contemporary electronic music. The system generates audio fragments based on text prompts provided in real time by the performer, enabling dynamic interaction. Guided by Musical Metacreation as a framework, this case study reframes generative AI as a co-creative agent rather than a mere style imitator. By fine-tuning Stable Audio Open on a dataset of the first author’s compositions and field recordings, this approach demonstrates the ethical and practical benefits of open-source solutions. Beyond showcasing the model’s creative potential, this study highlights the model’s significant challenges and the need for democratized tools with real-world applications.",Misagh Azimi (Victoria University of Wellington)*; Mo H. Zareei (Victoria University of Wellington),misagh.azimi@vuw.ac.nz; mo.zareei@vuw.ac.nz,misagh.azimi@vuw.ac.nz,paper,Misagh Azimi; Mo H. Zareei,Machine learning in musical performance,Interactive sound art and sound installations ; Music-related human-computer interaction ; Practice-based research approaches/methodologies/criticism,short,poster,in person,5,,posters-2,,nime2025_156.pdf,,,156.jpg,,"Generative AI, Improvisation, Real-time Interaction, Musical Metacreation, Democratizing AI",
165,Maximum Silence to Noise: Sound synthesis for responsive gestural control,"Modulation synthesis has been a foundational technique in the development of electronic musical instruments since their inception. This paper presents a novel approach to ring modulation synthesis, termed Maximum Silence to Noise (MSN), along with an associated method of gestural control facilitated by a pressure-sensitive multi-touch controller. The primary objective of this research is to develop an instrument capable of producing a broad and diverse range of audio spectra that can be expressively articulated through responsive touch-based interaction. Integrating the synthesis process with gestural parameter mapping is crucial for the performative capabilities of New Interfaces for Musical Expression (NIMEs). The technical development of the MSN instrument was subject to an iterative design process with mixed method evaluation. The usability and practical application of the MSN instrument was refined through performance experiences, which illustrate the effectiveness of the synthesis-gesture mappings in providing dynamic and expressive control over the diverse generated audio spectra.",Andrew Brown (Griffith University)*,andrew.r.brown@griffith.edu.au,andrew.r.brown@griffith.edu.au,paper,Andrew Brown,"Augmented, embedded and hyper instruments","Explorations of relationships between motion, gesture and music; Novel controllers, interfaces or instruments for musical expression",medium,poster,in person,5,,posters-2,,nime2025_165.pdf,,,165.jpg,,"Synthesis, gesture, interaction, expression",
167,SwimTunes: A gamified music performance system for co-creating with a novice audience,"This paper presents SwimTunes, a prototype game system designed for novice multi-user music-making in live performance settings. The system features a digital game and a public web app that allows audience members to participate using their mobile devices. After connecting via QR code, participants create and pilot virtual fish that generate music as they bump into one another. The performer then enters the game as a shark, using camera-based hand tracking to chase and consume the participants’ fish. The result is a performance dynamic that evolves from playful co-creation to one of gameful contest between the performer and audience. SwimTunes explores how this shifting interaction context can shape the instantiation of a set of musical parameters, and further how performers can harness gameplay metaphors to conduct live audiences in shared acts of musical expression. The paper details the design considerations and conceptual motivations that informed SwimTunes before describing its implementation via Node.js, Open Sound Control, Unreal Engine 5, and MetaSounds. It discusses technical challenges and opportunities unearthed during development and outlines future directions for the project and gamified music performance at large.",Thomas Studley (Queensland University of Technology)*,thomas.studley@qut.edu.au,thomas.studley@qut.edu.au,paper,Thomas Studley,Technologies or systems for collaborative music-making,"New music performance paradigms; Software frameworks, interface protocols, and data formats, for supporting musical interaction ; Web-based music performance",medium,poster,in person,5,,posters-2,,nime2025_167.pdf,,,167.jpg,,"Music game, gamified performance, audience participation, Unreal Engine 5, MetaSounds",
172,A Synthetic Cicada Soundscape Controlled by Breath,"This paper describes an interactive installation featuring a generative soundscape with breath control, that aims to capture the feeling of being in a forest full of cicadas. Inspired by a period of deep listening to cicada stridulations – in which I found the spatio-temporal pulsation of the sound mass reminiscent of breathing – this installation uses breath control to give a sense of breathing with a forest. The sound mass consists of multiple generative sources, each loosely modelled on an individual cicada stridulating. Each ‘cicada’ comprises a temporal hierarchy of pulse trains modulating a carrier frequency, with a simple sonic spatialization algorithm applied to give the sense of immersion in the sound mass. The algorithm is implemented in the Extempore audiovisual programming language, and utilizes an architecture in which each sonic parameter is inherently stochastic, much as the sound production mechanisms of actual Cicadas exhibit natural variation.",Toby Gifford (University of the Sunshine Coast)*,toby.gifford@gmail.com,toby.gifford@gmail.com,paper,Toby Gifford,Interactive sound art and sound installations,Sonic interaction design,short,poster,in person,5,,posters-2,,nime2025_172.pdf,,,172.jpg,,"generative, soundscape, ecoacoustics",
179,Augmentation of a Historical Harpsichord Keyboard Replica for Haptic-Enabled Interaction in Museum Exhibitions,"This paper describes the design and creation of an electronically augmented replica of a historical harpsichord keyboard with a typical 17th-century Italian layout to create a digital musical instrument. The keyboard was commissioned for exhibition in a musical instrument museum to enhance the visitor experience by providing an interface to digitised versions of instruments within the collection. The replica balances the competing de- mands of historical authenticity, public accessibility, and preser- vation. It replicates the original instrument’s tactile feedback and mechanical resistance using historically informed construction techniques. Optical sensors integrated within the mechanism capture the jacks’ motion data, enabling MIDI message gener- ation. This work situates itself within broader discussions on the role of technology in museums. A keyboard interface of this type o￿ers an opportunity to enhance visitor interaction with musical heritage while safeguarding delicate artefacts. The paper examines the keyboard’s design principles, technical implemen- tation, and implications, emphasising its contribution to public engagement and the long-term preservation of musical heritage.",Matthew Hamilton (Università di Bologna)*; Michele Ducceschi (Università di Bologna); Roberto Livi (Harpsichord Maker); Catalina Vicens (Museo di San Colombano); Andrew McPherson (Imperial College London),matthew.hamilton2@unibo.it; michele.ducceschi@unibo.it; robertvs.livi@gmail.com; catalina.vicens@genusbononiae.it; andrew.mcpherson@imperial.ac.uk,matthew.hamilton2@unibo.it,paper,Matthew Hamilton; Michele Ducceschi; Roberto Livi; Catalina Vicens; Andrew McPherson,"Discussions about the artistic, cultural, and social impact of NIME technology","Historical, theoretical or philosophical discussions about designing or performing with new interfaces ; Sensor and actuator technologies, including haptics and force feedback devices",medium,poster,in person,5,,posters-2,,nime2025_179.pdf,,,179.jpg,,"interactive exhibit, augmented instrument, optical sensing, harpsichord, tactile interface",
214,Hybrid Hand Drum: Where Tradition Resonates Through Technology,"This paper presents a hybrid frame drum that entangles acoustic and digital elements, merging the expressive depth of traditional percussion with the expanded possibilities of digital sound processing. Designed with 4 key principles - portability, hybridity, simplicity and low latency - the instrument allows for a fluid interplay between physical and real-time digital augmentation. Equipped with piezoelectric sensors, an FSR, and a DSP algorithm the drum extends its sonic landscape while preserving its acoustic presence. The design maintains an organic relationship between physical interaction and digital processing, and the three potentiometers provide intuitive yet flexible control, maintaining a balance between minimalism and expressivity. The bela platform ensures very low latency (7.55 ± 0.13 ms), making it highly responsive for live performance. User evaluation highlights its potential for expressive control and seamless hybrid performance while suggesting ergonomic and functional refinements. Future enhancements, such as feedback control and DSP presets, could deepen the entanglement between performer, instrument, and sound. This research explores the intersection of acoustic and digital sound, contributing to the design of hybrid instruments that blur the boundaries between physical resonance and electronic transformation, expanding possibilities for musical interaction.",Casper Preisler (AAU)*; Daniel Overholt (Aalborg University Copenhagen),cpreis24@student.aau.dk; dano@create.aau.dk,cpreis24@student.aau.dk,paper,Casper Preisler; Daniel Overholt,"Augmented, embedded and hyper instruments","Novel controllers, interfaces or instruments for musical expression ; Pedagogical perspectives and/or student projects in NIME-related courses",medium,poster,in person,5,,posters-3,,nime2025_214.pdf,,,214.jpg,,"augmented drum, digital musical instruments, electronic percussion, hybrid instrument",
184,Metabow: Gesture Mapping in Immersive Sonic Environments,"This paper presents the MetaBow, an augmented violin bow designed to control digital sound processing through real-time motion tracking. We discuss the challenges of mapping Inertial Measurement Unit (IMU) data to audio parameters in immersive multi-speaker environments and propose hybrid strategies using both direct mapping and machine learning models. We reflect on design choices, trade-offs, and performer experience, drawing from technical development and performance contexts. Three condensed case studies illustrate the system’s versatility in spatial and interactive musical performance.",Davor Vincze (Hong Kong Baptist University)*; Roberto Alonso (Hong Kong Baptist University); Peter Nelson (Hong Kong Baptist University),vincze@hkbu.edu.hk; robertoalonso@hkbu.edu.hk; peteracnelson@hkbu.edu.hk,vincze@hkbu.edu.hk,paper,Davor Vincze; Roberto Alonso; Peter Nelson,Practice-based research approaches/methodologies/criticism,"Augmented, embedded and hyper instruments ; Machine learning in musical performance; Musical mapping strategies",short,poster,in person,5,,posters-3,,nime2025_184.pdf,,,184.jpg,,"augmented violin bow, machine learning and gesture, mapping strategies in immersive environments",
215,Embedded Comparo: Small DSP Systems Side-by-Side,"This paper presents a comparative analysis of four embedded platforms designed for real-time audio processing: Bela, Daisy, OWL, and Raspberry Pi. These platforms have become integral tools in the field of digital musical instrument design, offering a variety of workflows, programming environments, and deployment methods. Although each system carries its own distinct strengths and constraints, the current workflow to embed DSP code across multiple devices lacks standardized approaches. To address this challenge, we develop a methodology that focuses on deploying Pure Data patches across all four platforms. Our study is structured around four test patches. Our findings highlight the trade-offs in latency, processing power, and memory constraints across the selected platforms. As a result, we propose a streamlined workflow to deploy Pd patches on each board using Plugdata, the Heavy Compiler, and their respective Web IDEs. As an ongoing contribution to the NIME community, we document our methodologies, workflows, and best practices in an open source repository, which serves as a continuously evolving resource for future research in the hands of musicians, researchers, and developers working with embedded musical systems.",Francesco Di Maggio (Eindhoven University of Technology)*; Bart Hengeveld (Eindhoven University of Technology); Atau Tanaka (Goldsmiths),f.di.maggio@tue.nl; b.j.hengeveld@tue.nl; a.tanaka@gold.ac.uk,f.di.maggio@tue.nl,paper,Francesco Di Maggio; Bart Hengeveld; Atau Tanaka,"Augmented, embedded and hyper instruments",Evaluation and user studies of commercially available “off the shelf” interfaces,long,poster,in person,5,Have been updated by Francesco about in-person attandace. ,posters-3,,nime2025_215.pdf,,,215.jpg,,"Embedded Platforms, DSP, Plugdata, Bela, Daisy, OWL, Raspberry Pi",
228,Acoustic-digital hybrid synthesizer,"This paper explores the design and evaluation of an acoustic-digital hybrid instrument that aims to address key criticisms of Digital Musical Instruments (DMIs), particularly the separation of control and sound generation. By integrating an interactable physical string with coupled Finite Difference Schemes (FDS) for physical modeling synthesis, the instrument creates a tactile and responsive playing experience. The instrument was evaluated through a mixed-methods approach, combining qualitative think-aloud protocols with the Musician’s Perception of the Experiential Quality of Musical Instruments Questionnaire (MPX-Q). Results indicate that the instrument fosters curiosity and creativity but highlights challenges in achieving traditional acoustic playability, such as latency and perceptual dissonance. These findings emphasize the potential and limitations of acoustic-digital hybrids in reuniting control and sound, offering valuable insights for future developments in musical interface design.",Levin Schnabel (Aalborg University)*; Dan Overholt (Aalborg University),lschna23@student.aau.dk; dano@create.aau.dk,lschna23@student.aau.dk,paper,Levin Schnabel; Dan Overholt,Evaluation and user studies of new interfaces for musical expression,"Novel controllers, interfaces or instruments for musical expression ; Sensor and actuator technologies, including haptics and force feedback devices",long,poster,in person,5,,posters-3,,nime2025_228.pdf,,,228.jpg,,"Acoustic-digital hybrid instruments, Physical modeling synthesis, Controller-generator paradigm",
234,The Drum Machine of Tao,"The Drum Machine of Tao (Tao) is a machine learning–based system that reverse-engineers sequencer parameters and one-shot percussive samples from drum loops, restoring low-level editability to sampled loops that would otherwise be frozen in audio waveforms. The philosophy behind this system is inspired from Taoism: that which returns to its primal state is the great Way of Tao. In this paper, we present the system design of Tao, which includes a state-of-the-art drum source separation model, a sequencer parameter estimation model, and a bespoke one-shot sample extraction algorithm that leverages differentiable audio synthesis. Results from a prototype are available for listening.","Xiaowan Yi (Centre for Digital Music, Queen Mary Univeristy of London)*; Mathieu Barthet (Centre for Digital Music, Queen Mary University of London)",x.yi@qmul.ac.uk; m.barthet@qmul.ac.uk,x.yi@qmul.ac.uk,paper,Xiaowan Yi; Mathieu Barthet,Machine learning and artificial intelligence in NIMEs,"Novel controllers, interfaces or instruments for musical expression",short,poster,in person,5,,posters-3,,nime2025_234.pdf,,,234.jpg,,"drum machine, sequencer parameter estimation, one-shot sample extraction",
254,Designing A Tangible Rhythmic Interface for Digital Drum Talk,"I propose a tangible user interface and communication protocol for computer mediated rhythm-based interaction for educational applications (music, language, mathematics). Thinking beyond the paradigm of keyboard-and-screen-based interfaces, this project is based on previous work on the Drumball. By converting rhythmic input into multimodal output, it creates an entangled ecosystem where the human body, digital musical instruments, and the Internet of Things intersect. Such a digital orality system could offer parents and practitioners a novel method for introducing children to literacy, STEAM skills and multimodal communication in the early years. I present design iterations of 1) a tangible rhythmic interface for digital drum talk inspired by the style of play of the Djembe, 2) a protocol for sending piezo sensor outputs over a custom PCB shield, which can be recognized across multiple platforms and web-based environments without additional customization; and 3) a suite of rhythm-based learning games using the Alphariddims multimodal symbol system based on the Morse code. I argue that such a culturally-grounded approach to music technology design provides a viable avenue for the preservation and revitalization of the vibrant, yet intangible, cultural heritage and traditions of the African talking drum cultural systems.",Pierre-Valery Tchetgen (Northeastern University)*,p.tchetgen@northeastern.edu,p.tchetgen@northeastern.edu,paper,Pierre-Valery Tchetgen,"Historical, theoretical or philosophical discussions about designing or performing with new interfaces",Gesture to sound mapping,long,poster,in person,5,,posters-3,,nime2025_254.pdf,,,254.jpg,,"drummology,  talking drum controller,  rhythmic interaction, drum language communication, embodied learning",
276,Luna: An AR Musical Instrument on the Meta Quest 2,"Head-mounted augmented reality (AR) computers present the opportunity to develop new musical interfaces that would be impossible to build physically or with conventional computing devices. Unfortunately, typical computer music tools have not been easy to apply within AR development tool chains. Integrating standard computer music tools in AR development would allow more rapid prototyping of new instrument ideas and transfer of knowledge from experienced computer musicians. The goal of this paper is to demonstrate that AR digital musical instruments can be developed using libpd, the library version of the standard computer music environment Pure Data. We present a case study of an AR instrument developed for the Meta Quest 2 integrating libpd in the AR development tool-chain for the interactive audio components. The iterative development process was tracked through autoethnographic reflections and analysed with thematic analysis. We found that Pure Data was an effective way to develop audio interactions on the Quest 2 and that the hand tracking on this platform was capable of complex gestural interactions. This work could enable a broader community of computer musicians to explore AR NIME development, taking advantage of the unique affordances of this medium.",Samuel Dietz (Australian National University); Charles Patrick Martin (Australian National University)*,samuel.dietz@anu.edu.au; charles.martin@anu.edu.au,charles.martin@anu.edu.au,paper,Samuel Dietz; Charles Patrick Martin,"Extended reality environments: augmented, virtual, mixed reality","Music-related human-computer interaction ; Software frameworks, interface protocols, and data formats, for supporting musical interaction",medium,poster,in person,5,,posters-3,,nime2025_276.pdf,,,276.jpg,,"augmented reality, free hand interaction, first-person",
280,ChuMP: The Zen and Art of Package Management,"ChuMP stands for “ChucK Manager of Packages”, designed to automate the process of installing, upgrading, and removing software components for the ChucK programming ecosystem. ChuMP manages libraries, tools, audio and graphics plugins in a centralized, structured, and versioned manner. This project originated out of the recent ChucK development “renaissance” alongside a growing user community, now entering its third decade. The time, as the ChucK slogan goes, is now. What began as a practical project has expanded into broader reflections on tool-building, service, and community. As we labored on what seemed like a “no-brainer” tool that everyone wanted but that no one wanted to build, questions arose: “how did we get here?”, “what is the role of service-based tool-building in our field–and what, if any, is its research value?”—in short, “should we even write a paper about a package manager?”. Meanwhile, we couldn’t help but notice that the act of creating a package manager seems to unify not only disparate software fragments, but also something of community. In other words, there may be more than meets the eye. This paper chronicles the making of a package manager and all that goes along with it. This is the story of ChuMP.",Nicholas Shaheed (Stanford University)*; Ge Wang (Stanford University),nshaheed@ccrma.stanford.edu; ge@ccrma.stanford.edu,nshaheed@ccrma.stanford.edu,paper,Nicholas Shaheed; Ge Wang,"Software frameworks, interface protocols, and data formats, for supporting musical interaction",Practice-based research approaches/methodologies/criticism,long,poster,in person,5,,posters-3,,nime2025_280.pdf,,,280.jpg,,"package manager, philosophy, community, craft, ""is this research?""",
284,SMucK: Symbolic Music in ChucK,"SMucK (Symbolic Music in ChucK) is a library and workflow for creating music with symbolic data in the ChucK programming language. It extends ChucK by providing a framework for symbolic music representation, playback, and manipulation. SMucK introduces classes for scores, parts, measures, and notes; the latter encode musical information such as pitch, rhythm, and dynamics. These data structures allow users to organize musical information sequentially and hierarchically in ways that reflect familiar conventions of Western music notation. SMucK supports data interchange with formats like MusicXML and MIDI, enabling users to import notated scores and performance data into SMucK data structures. SMucK also introduces SMucKish, a compact high-level input syntax, designed to be efficient, human-readable, and live-codeable. The SMucK playback system extends ChucK’s strongly-timed mechanism with dynamic temporal control over real-time audio synthesis and other systems including graphics and interaction. Taken as a whole, SMucK’s design philosophy treats symbolic music data not only as static representations but also as mutable, recombinant building blocks for algorithmic and interactive processing. By integrating symbolic music into a strongly-timed, concurrent programming language, SMucK’s workflow goes beyond data representation and playback, and opens new possibilities for algorithmic composition, instrument design, and musical performance.",Alexander Han (Stanford University)*; Kiran Bhat (Stanford University); Ge Wang (Stanford University),tae1han@stanford.edu; kvbhat@stanford.edu; ge@ccrma.stanford.edu,tae1han@stanford.edu,paper,Alexander Han; Kiran Bhat; Ge Wang,"Software frameworks, interface protocols, and data formats, for supporting musical interaction",Interface protocols and data formats supporting musical interaction,long,poster,in person,5,,posters-3,,nime2025_284.pdf,,,284.jpg,,"ChucK, programming, notation, symbolic music",
300,GraviTone: A Tangible Musical Interface using Gravity Well for Sound and Music Creation,"We propose a new musical interface, “GraviTone” that produces sounds and musical compositions via interaction with spherical objects in a gravity-well benchtop setup. The interface consists of several spherical objects in different colours orbiting around the centrally placed static object on the spacetime fabric. The spherical objects are launched at a certain angle from the setup's periphery, and the objects' motion is tracked by an overhead camera and mapped to different parameters to generate sound. In GraviTone, users can experience sounds generated from objects’ parameters and have control over different configurations. We use the setup to send Open Sound Control (OSC) signals and Musical Instrument Digital Interface (MIDI) signals to map different sounds and musical scales (Indian classical ragas and Western classical scale). We mapped the moving objects’ parameters to control synth parameters, VSTs, and DAWs. Users can also route generated MIDI data into DAWs using preset configurations or customizing their frameworks for sound generation. We also generate real-time visuals corresponding to the object's movements for further immersion and interactivity. This integrated interface combines various domains like musical mathematics, sonification techniques, sound synthesis, and sound design for live music creation and real-time audiovisual composition.",Kratika Jain (Indian Institute of Technology - Kanpur); Allwin  Williams (Indian Institute of Technology - Kanpur); Akhilesh Kumar Bhagat (Indian Institute of Technology - Kanpur); Sukanth K (Indian Institute of Technology - Kanpur); Arunav Rajesh (Indian Institute of Technology - Kanpur); Prashant  Pal (Indian Institute of Technology - Kanpur); Krishanan Chandran (TU Dresden); Rajashekhar  V S  (Indian Institute of Technology - Kanpur)*; Anandu Ramesh (Indian Institute of Technology - Kanpur); Gowdham Prabhakar  (Indian Institute of Technology - Kanpur),jainkratika498@gmail.com; allwinwilliams.info@gmail.com; akhileshkrbhagat1@gmail.com; sukanth.original@gmail.com; arunavr23@iitk.ac.in; prashpal23@iitk.ac.in; krishnan.chandran@tu-dresden.de; raja23@iitk.ac.in; anandurpt23@iitk.ac.in; gowdhampg@iitk.ac.in,raja23@iitk.ac.in,paper,Kratika Jain; Allwin  Williams; Akhilesh Kumar Bhagat; Sukanth K; Arunav Rajesh; Prashant  Pal; Krishanan Chandran; Rajashekhar  V S; Anandu Ramesh; Gowdham Prabhakar,Interactive sound art and sound installations,Musical mapping strategies ; Sonic interaction design ; Technologies or systems for collaborative music-making,medium,poster,in person,5,,posters-3,,nime2025_300.pdf,,,300.jpg,,"data sonification, musical mapping, audio synthesis, musical mathematics, sound design",
68,Between Garment and Prosthesis:  The Design of an E-Textile Musical Interface,"This paper presents Noisy Flesh, an e-textile musical interface designed to control sound through body movement in performance. Unlike traditional wearable interfaces that function as garments, this work reimagines the textile interface as a prosthetic extension that augments the performer’s body. The paper discusses both the design of the interface and its sonification method, emphasising how the flexibility of e-textiles can transform bodily movement and shape interactive experiences. This work explores the potential of e-textile interfaces to challenge conventional notions of wearability and embodiment in performance.","Qiaosheng Lyu (City, University of Hong Kong)*; Ryo Ikeshiro (	City, University of Hong Kong)",qslyu2-c@my.cityu.edu.hk; ryo.ikeshiro@cityu.edu.hk,qslyu2-c@my.cityu.edu.hk,paper,Qiaosheng Lyu; Ryo Ikeshiro,"Novel controllers, interfaces or instruments for musical expression","Entangled NIME: Intertwined, multilayer contexts in NIME research; Explorations of relationships between motion, gesture and music; Gesture to sound mapping",short,oral,remote,5,,papers-1,Collective and Embodied,nime2025_68.pdf,,,68.jpg,,"E-textile, Body Instrument, Wearable Interface, Musical Prosthesis",
78,Towards Neurodiverse Sensemaking: Pluralizing Agency in Wearable Music and Participatory Workshopping,"We, a team of teachers and researchers, share examples of collectively playable instruments that challenge normative assumptions about intention and agency in digital musical instruments. These instruments enliven neurodiverse sensemaking in participatory design and STEAM learning. Through a multiyear research-practice partnership (RPP), we collaborated with teaching fellows to co-design a curriculum for neurodiverse middle school students that activates computational thinking (CT). This collaboration led to a web-based, quasi-modular interface connected to wearable music sensors. We situate our work within the growing literature on participatory design of collaborative accessible digital musical instruments (CADMIs). We describe how our co-design methods address the complex demands of ecosystemic thinking, sensitive to the varied entanglements that complicate traditional human-computer interaction (HCI) design and evaluation methods. Our pedagogical and methodological approach diverges from deficit-focused strategies that aim to develop neurotypical communication skills in neurodivergent individuals. Instead, we promote cross-neurotype collaboration without presuming a single mode of ""correct"" communication. Furthermore, we surface the potential of CADMIs by linking this notion to a pluralization of agency that extends beyond one-to-one body-sensor relationships. We develop accessible instruments within neurodiversity and autism contexts, avoiding reification of mindbody relations and recognizing them as dynamic, field-like, and embedded in facilitative relations for these communities.",Seth Thorn (Arizona State University)*; Anani Vasquez (Neurodiversity Education Research Center); Corey Reutlinger (Arizona State University); Margarita Pivovarova (Arizona State University); Mirka Koro (Arizona State University),seth.thorn@asu.edu; avasquez@neurodiversitycenter.org; creutlin@asu.edu; Margarita.Pivovarova@asu.edu; Mirka.Koro@asu.edu,seth.thorn@asu.edu,paper,Seth Thorn; Anani Vasquez; Corey Reutlinger; Margarita Pivovarova; Mirka Koro,Accessibility and interfaces for musical expression for people with special needs,"Entangled NIME: Intertwined, multilayer contexts in NIME research; Pedagogical perspectives and/or student projects in NIME-related courses",long,oral,remote,15,,papers-1,Collective and Embodied,nime2025_78.pdf,,,78.jpg,,"Neurodiversity, Accessible Digital Musical Instrument, Computational Thinking, Wearable Music, Entanglement",
279,Hyperwilding: Sonic Perplexity as Urban Acupuncture to Promote Environmental Kinship,"This paper invites discussion of how sound art installations, specifically those situated in urban environments, can serve as respite from urban stressors as well as advocate for increased awareness and engagement of acoustic ecology. The author invokes the theoretical framework of Karen Barad to juxtapose the Urban Acupuncture movement with the Solarpunk ethos, arguing that sound installations may be crafted as agential cuts to the entangled relationship of humans and their built environments. This paper surveys sound artists that have specifically engaged the urban space—an environment that one could argue is more “natural” to humans than the remote picturesque landscapes commonly associated with the concept. Finally, the author describes some of his past sonic interventions and expounds on his current project, “Standing Wave,” commissioned by the city government and non-profits to address Extreme Urban Heat. He discusses how this installation, coupled with targeted community engagement through “Environmental Listening” workshops, urges us to rethink the temporality of intervention, recognizing that long-term strategies, while not immediate solutions, are crucial for future cooling and remediating the effects of climate change.",Shomit Barua (Moire Intermedia)*,shomitbarua@gmail.com,shomitbarua@gmail.com,paper,Shomit Barua,"Discussions about the artistic, cultural, and social impact of NIME technology","Entangled NIME: Intertwined, multilayer contexts in NIME research; Interactive sound art and sound installations ; Practice-based research approaches/methodologies/criticism",medium,oral,remote,10,,papers-1,Collective and Embodied,nime2025_279.pdf,,,279.jpg,,"Ecoacoustics, Acoustic Ecology, Sustainability, Public Art, Circuitbending, Solar Power, Analog Synthesis",
11,Simulated EEG-Driven Audio Information Mapping Using Inner Hair-Cell Model and Spiking Neural Network,"This study presents a framework for mapping audio information into simulated neural signals and dynamic control maps. The system is based on a biologically-inspired architecture that traces the sound pathway from the cochlea to the auditory cortex. The system transforms acoustic features into neural representations by integrating Meddis's Inner Hair-Cell (IHC) model with spiking neural networks (SNN). The mapping process occurs in three phases: the IHC model converts sound waves into neural impulses, simulating hair cell mechano-electrical transduction. These impulses are then encoded into spatio-temporal patterns through an Izhikevich-based neural network, where spike-timing-dependent plasticity (STDP) mechanisms enable the emergence of activation structures reflecting the acoustic information's complexity. Finally, these patterns are mapped into both EEG-like signals and continuous control maps for real-time interactive performance control. This approach bridges neural dynamics and signal processing, offering a new paradigm for sound information representation. The generated control maps provide a natural interface between acoustic and parametric domains, enabling applications from generative sound design to adaptive performance control, where neuromorphological sound translation explores new forms of audio-driven interaction.","Pasquale Mainolfi (Conservatorio G. Martucci, Salerno IT)*",mnlpql@gmail.com,mnlpql@gmail.com,paper,Pasquale Mainolfi,Musical mapping strategies,"Machine learning in musical performance; New music performance paradigms; Software frameworks, interface protocols, and data formats, for supporting musical interaction",long,oral,remote,15,,papers-3,Body and Motion,nime2025_11.pdf,,,11.jpg,,"Audio Information Mapping, Inner Hair-Cell Model, Spiking Neural Network, EEG, Neural Control Signals",
107,EMMA: Enhancing Real-Time Musical Expression through Electromyographic Control,"This paper presents the Electromyographic Music Avatar (EMMA), a digital musical instrument (DMI) designed to enhance real-time sound-based composition through gestural control. Developed as part of a doctoral research project, EMMA combines electromyography (EMG) and motion sensors to capture nuanced finger, hand, and arm movements, treating each finger as an independent instrument. This approach bridges embodied performance with computational sound generation, enabling expressive and intuitive interaction. The system features a glove-based design with EMG sensors for each finger and motion detection for the wrist and arm, allowing seamless control of musical parameters. By addressing key challenges in DMI design, such as action-sound immediacy and performer-instrument dynamics, EMMA contributes to developing expressive and adaptable tools for contemporary music-making.",João  Coimbra (Aveiro University/INET-md)*; Luís Aly (Escola Superior de Media Artes e Design/IPP); Henrique  Portovedo (Aveiro University/INET-md); Sara Carvalho (Aveiro University/INET-md); Tiago  Bolaños (Instituto Superior Técnico/IT),jpcoimbra@ua.pt; luisaly@esmad.ipp.pt; henriqueportovedo@ua.pt; scarvalho@ua.pt; tiago.bolanos@tecnico.ulisboa.pt,jpcoimbra@ua.pt,paper,João  Coimbra; Luís Aly; Henrique  Portovedo; Sara Carvalho; Tiago  Bolaños,"Novel controllers, interfaces or instruments for musical expression","Gesture to sound mapping; Music-related human-computer interaction ; Sensor and actuator technologies, including haptics and force feedback devices",short,oral,remote,5,,papers-3,Body and Motion,nime2025_107.pdf,,,107.jpg,,"Gestural interface, Electromyography, Digital Musical Instrument, Human-Computer Interaction, Performance",
229,From Performance to Installation: How Interactive Reinforcement Learning Reframes the Roles of Performers and Audiences,"This paper explores how interactive reinforcement learning (IRL) reconfigures the roles of performers and audiences in audiovisual performance and immersive installation. We adapt the Co-Explorer (a software tool originally developed for musical co-creation) to audiovisual immersive contexts and examine its creative potential using a reflexive research-creation approach. Our study reveals how IRL splits the role of the performer into three distinct positions: (1) the designer, who defines the parametric space; (2) the guide, who reinforces the agent’s behavior; and (3) the machine performer, whose actions are shaped by interactive training. As IRL introduces agency into the creative process, it transforms traditional notions of authorship and control, enabling unexpected emergent outcomes. By showcasing an interactive installation/performance, we further explore how audiences contribute to collective creation through reinforcement-based interaction. Our findings underscore the challenges of balancing the temporality of IRL with the demands of public-facing works and of adapting RL-based systems to different exhibition contexts. Our work contributes to the discourse on co-creative systems, emphasizing the evolving roles of artists, artificial agents, and audiences in hybrid creative ecosystems.",Danny Perreault (UQAM); Victor Drouin-Trempe (UQAM); Vincent Cusson (UQAM)*; David Drouin (UQAM); Sofian Audry (UQAM);,cusson.v@gmail.com; perreault.dan@gmail.com; audry.sofian@uqam.ca; victordrouint@gmail.com; drouin.david@courrier.uqam.ca,cusson.v@gmail.com,paper,Danny Perreault; Victor Drouin-Trempe; Vincent Cusson; David Drouin; Sofian Audry;,Machine learning and artificial intelligence in NIMEs,Machine learning in musical performance; Music-related human-computer interaction ; Practice-based research approaches/methodologies/criticism,medium,oral,remote,10,Joining from EU zone not CAN - OK moved to a different session,papers-3,Body and Motion,nime2025_229.pdf,,,229.jpg,,"art Installation, interactive machine learning, reinforcement learning, audiovisual performance",
90,Longevity of Deep Generative Models in NIME: Challenges and Practices for Reactivation,"In this paper, we present an investigation into the longevity, reproducibility, and documentation quality of Deep Generative Models (DGMs) introduced in previous editions of the NIME conference. We begin by assessing whether DGM presented at NIME are still available in terms of code, data, and weights; afterward, we present the recreation process of seven unavailable models, to the end of investigation of the issues related to longevity and documentation. We examine the availability and completeness of resources needed to recreate DGM models, and discuss specific challenges encountered during such recreation. Drawing from  this experience, we highlight key obstacles that hinder the long-term viability and reuse of DGMs in the NIME context, and propose guidelines to improve their documentation and future reuse within the community.","Isaac Clarke (The Hong Kong University of Science and Technology (Guangzhou))*; Francesco Ardan  Dal Rí (Department of Information Engineering and Computer Science, University of Trento); Raul Masu (The Hong Kong University of Science and Technology (Guangzhou))",ijclarke590@connect.hkust-gz.edu.cn; francesco.dalri-2@unitn.it; raul@raulmasu.org,ijclarke590@connect.hkust-gz.edu.cn,paper,Isaac Clarke; Francesco Ardan  Dal Rí; Raul Masu,Machine learning and artificial intelligence in NIMEs,"Practice-based research approaches/methodologies/criticism ; Software frameworks, interface protocols, and data formats, for supporting musical interaction",medium,oral,remote,10,,papers-4,"Environment, Sustainability, Longevity",nime2025_90.pdf,,,90.jpg,,"longevity, reuse, recreation, machine learning, documentation, sustainability",
37,Chimera: Prototyping a New DMI for Congenital One-Handed Musicianship Through an Autoethnographic Lens,"Chimera is a Digital Musical Instrument (DMI) prototype developed through an autoethnographic lens. That is, a lens shaped by congenital one-handedness as well as extensive experience as both a disabled player of standard instruments and a designer of DMIs for other players. Leveraging Eurorack synthesizer modules as a flexible prototyping toolkit enables an iterative prototyping process that explores the distinctive possibilities of one-handed musicianship. Reflection on a three-month period of iteration and refinement highlights a series of design issues, but also the interconnectedness of physical impairments, and the difficulties of designing for a body in flux. Some directions for future work are outlined. Finally, by discussing the various entangled layers of this instrument prototype, and starting to tease out what Koutsomichalis calls its “stories of a sort”, this paper contributes an until now underrepresented perspective to the dialogue around accessible and inclusive musical instrument design, and disability and musicianship more broadly.",Mat Dalgleish (Staffordshire University)*,mat.dalgleish@staffs.ac.uk,mat.dalgleish@staffs.ac.uk,paper,Mat Dalgleish,Accessibility and interfaces for musical expression for people with special needs,"Discussions about the artistic, cultural, and social impact of NIME technology ; Entangled NIME: Intertwined, multilayer contexts in NIME research",long,oral,remote,15,,papers-6,Accessibility,nime2025_37.pdf,,,37.jpg,,"DMI prototyping, physical disability, autoethnography, Eurorack, entangled layers",
88,The Slide-A-Phone: a Tactile Accessible Musical Instrument,"AMIs, Inclusive design, autobiographical design This paper details the design and development of the Slide-A-Phone, an Accessible Musical Instrument (AMI). The first author's spinal cord injury in 2004 hindered their ability to play traditional instruments, which motivated the development of the Slide-A-Phone. The Slide-A-Phone utilises tactile interfaces coupled with analogue and digital sensors to replicate the playability and expressive control of a saxophone, the instrument the first author used to play before the incident. The design process incorporated phenomenological perspectives and a blend of design methodologies, with the specific goal of fostering a robust musician-instrument relationship. We report insights into how personal experiences shape design and functionality and the importance of accessible instruments in enabling creative practice and performance for individuals with limited functionality. We also describe the design and technical implementation of the Slide-A-Phone evaluate the instrument's effectiveness and reflect on its potential to enhance musical engagement, social connections, cultural participation, and professional development.",Andrew McMillan (University of Auckland)*; Fabio Morreale (The University of Auckland),drewmcm@blackbridge.co.nz; f.morreale@auckland.ac.nz,drewmcm@blackbridge.co.nz,paper,Andrew McMillan; Fabio Morreale,Accessibility and interfaces for musical expression for people with special needs,Evaluation and user studies of new interfaces for musical expression ; Practice-based research approaches/methodologies/criticism ; User studies and evaluations of NIMEs,medium,oral,remote,10,,papers-6,Accessibility,nime2025_88.pdf,,,88.jpg,,"AMIs, Inclusive design, autobiographical design, Evaluation",
21,Entangling with Light and Shadow: layers of interaction with the pattern organ,"This paper explores the design and use of a camera-based digital musical instrument as a thinking tool for considering entangled, post-human perspectives. The design of the pattern organ, inspired by experimental optical sound-on-film practices, employs a method of visual-to-audio synthesis that responds closely to the material behaviours captured by its camera input. Drawing on findings from exploratory workshops and short material experiments, we describe how interactions emerge and are shaped by both the physical configuration of the instrument and the material behaviours captured by its camera. We consider how frugal mappings and the ‘rawness’ of data can give rise to instruments whose inputs remain open to material complexity, extending the sound engine beyond their enclosures. In the case of the pattern organ, this complexity emerges through overlapping and interfering interactions, where structural forms, human influence, light, shadows, lens distortions, and system quirks all contribute to the shifting harmonic content of the wavetable. We reflect on the instrument as a fluid assemblage, composed of human and non-human entanglements, encouraging us to think beyond traditional notions of human-centred control.",Jasmine Butt (UWE)*; Benedict Gaster (UWE (University of the West of England)); Nathan Renney (UWE (University of the West of England)); Maisie Palmer (UWE (University of the West of England)),jasmine2.butt@live.uwe.ac.uk; Benedict.Gaster@uwe.ac.uk; Nathan.Renney@uwe.ac.uk; Maisie3.Palmer@live.uwe.ac.uk,jasmine2.butt@live.uwe.ac.uk,paper,Jasmine Butt; Benedict Gaster; Nathan Renney; Maisie Palmer,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Evaluation and user studies of new interfaces for musical expression ; Music-related human-computer interaction ; Novel controllers, interfaces or instruments for musical expression",long,oral,remote,15,,papers-7,Entangled NIME,nime2025_21.pdf,,,21.jpg,,"Visual-to-Audio Synthesis, Optical Sound, Entanglement, Material Drift, DMI Design",
75,(De)Constructing Timbre at NIME: Reflecting on Technology and Aesthetic Entanglements in Instrument Design,"Timbre, pitch, and timing are often relevant in digital musical instrument (DMI) design. Compared with the latter two, timbre is neither easy to define nor discretise when negotiating audio representations and gesture-sound mappings. We conduct a corpus assisted discourse analysis of ``timbre'' in all NIME proceedings to date (2001--2024). Combining this with a detailed review of 18 timbre-focused papers at NIME, we examine how definitions of timbre and timbre interaction methods are constructed through, for instance, Wessel's numerical timbre control space, synthesis tools and programming languages, machine learning and AI approaches, and other trends in digital lutherie practices. While acknowledging the practical utility of technical constructions of timbre in NIME (and other digital music research communities), we contribute discussion on the entanglement of technology and aesthetics in instrument design, which constitutes what ``timbre'' becomes in NIME research and reflect on the tension between technoscientific and constructivist understandings of timbre: how DMIs and musical practices have been reconstituted around particular timbral values operationalised in NIME. In response, we propose ways that the NIME community can embrace more critical approaches and awareness to how our methods and tools shape and co-create our notions of timbre, as well as other musical concepts, connecting more openly with diverse types of sonic phenomena.",Charalampos Saitis (Queen Mary University of London)*; Courtney N. Reed (Loughborough University London); Ashley Laurent Noel-Hirst (Queen Mary University of London); Giacomo Lepri (University of Genoa); Andrew McPherson (Imperial College London),c.saitis@qmul.ac.uk; c.n.reed@lboro.ac.uk; a.l.noel-hirst@qmul.ac.uk; giacomo.lepri@edu.unige.it; andrew.mcpherson@imperial.ac.uk,c.saitis@qmul.ac.uk,paper,Charalampos Saitis; Courtney N. Reed; Ashley Laurent Noel-Hirst; Giacomo Lepri; Andrew McPherson,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Discussions about the artistic, cultural, and social impact of NIME technology ; Historical, theoretical or philosophical discussions about designing or performing with new interfaces ; Music-related human-computer interaction",long,oral,remote,15,,papers-7,Entangled NIME,nime2025_75.pdf,,,75.jpg,,"Timbre, Entanglement, Metaphor, Reification, Constructivism",
137,The Imperfect Copy: Role Playing Reenactments of Historical Electronic Sound Instruments,"Reenactment forms a unique method of exploring the social, political, historical, conceptual, contextual and other aspects of electronic sound instruments from the past, without necessarily reproducing the instrument’s physical, functional or sonic characteristics. Rather, the reenactment presents a novel instrument, realized through contemporary means, reflecting on contemporary concerns and within a contemporary context. We find reenactment complementary to conservation, maintenance, reconstruction and emulation in working with archival and museum objects. Our paper presents an analytic framework developed for use in workshop scenarios. This series of questions helps determine and understand which aspects of an instrument might be reenacted. To illustrate the process in action, we describe an example workshop wherein participants use methods of media archaeology, design fiction and role playing to imagine and reenact new features, affordances, contexts and applications of electronic instruments from a museum exhibition.",Derek Holzer (KTH Royal Institute of Technology in Stockholm)*; Henrik Frisk (KMH Royal College of Music Stockholm); André Holzapfel (KTH Royal Institute of Technology in Stockholm),idholzer@kth.se; henrik.frisk@kmh.se; holzap@kth.se,idholzer@kth.se,paper,Derek Holzer; Henrik Frisk; André Holzapfel,"Historical, theoretical or philosophical discussions about designing or performing with new interfaces","Discussions about the artistic, cultural, and social impact of NIME technology ; Entangled NIME: Intertwined, multilayer contexts in NIME research; Pedagogical perspectives and/or student projects in NIME-related courses",long,oral,remote,15,Update: switch to remote now Photo credit on paper gallery.,papers-8,Historical and Cultural Reflections,nime2025_137.pdf,,,137.jpg,"Pär Fredin / The Swedish Museum of Performing Arts, used with
permission.","Media archaeology, workshop, reenactment, design fiction, role playing",
195,The Memory Cloud: Personal media libraries as affordance and constraint,"The Memory Cloud is a musical instrument that uses a player’s own library of personal recordings as sonic material. This paper presents the design of the instrument, situating it within sustainability HCI studies and constraints-based design, before describing the instrument being used by two musicians in a professional context. Over 2000 sounds from the musician's personal cloud library, dating back over 10 years, were placed in the instrument as the only sonic material available for exploring. I argue that a radically small scale and personal approach could be one strategy for addressing the issues of longevity in NIME, and I suggest that using personal media libraries presents a potential affordance and constraint for musical instrument design.",Yann Seznec (KTH Royal Institute of Technology)*,yannse@kth.se,yannse@kth.se,paper,Yann Seznec,"Novel controllers, interfaces or instruments for musical expression","Discussions about the artistic, cultural, and social impact of NIME technology ; Entangled NIME: Intertwined, multilayer contexts in NIME research",medium,oral,remote,10,,papers-8,Historical and Cultural Reflections,nime2025_195.pdf,,,195.jpg,,"instrument, sustainability, sound, music, experimental",
232,AR Matchmaking: The Compatibility of Musical Instruments with an AR Interface,"Augmented Reality (AR) interfaces offer new possibilities for musical expression by extending the capabilities of acoustic, electronic, and electroacoustic instruments. This study investigates the usability of the ARCube, an AR-based spatial audio controller, with twelve distinct musical instruments played by experienced musicians. We identify usability challenges specific to certain instruments, particularly for two-handed playing, as well as issues related to gesture recognition and cube stability. Our analysis shows that interaction patterns, such as cube placement, sound effect usage, and gesture strategies, vary significantly between instruments. These differences are driven by the physical form of the instruments, the required playing techniques, and user expectations for control and responsiveness. Based on these insights, we suggest directions for developing adaptable AR interfaces that better accommodate diverse instruments and support broader integration of AR technologies into musical practice.",Hyunkyung Shin (Georgia Institute of Technology)*; Henrik von Coler (Georgia Institute of Technology),luhee2458@gmail.com; hvc@gatech.edu,luhee2458@gmail.com,paper,Hyunkyung Shin; Henrik von Coler,Evaluation and user studies of new interfaces for musical expression,"Augmented, embedded and hyper instruments ; Extended reality environments: augmented, virtual, mixed reality; Music-related human-computer interaction",long,oral,remote,15,,papers-9,Extended Reality,nime2025_232.pdf,,,232.jpg,,"Augmented Reality Interface, Spatial Sound, User Experience Evaluation",
8,Eco-Sonic Interfaces for Embodied AI Sound Exploration,"Neural Tides is a neural network-based granular synthesizer that examines plastiglomerates—hybrid formations of plastic and organic material in marine environments. The system maps sound grains from oceanic field recordings to a navigable latent space using autoencoders and clustering techniques, controlled via hand gestures and touch. This interface physically connects performers with sonic representations of anthropogenic material transformations in coastal environments.",Sabina Hyoju Ahn (University of California Santa Barbara)*; Ryan Millett (	University of California Santa Barbara); Seyeon Park (Artist),sabina_ahn@ucsb.edu; rmillett@ucsb.edu; seyparc@gmail.com,sabina_ahn@ucsb.edu,paper,Sabina Hyoju Ahn; Ryan Millett; Seyeon Park,"Entangled NIME: Intertwined, multilayer contexts in NIME research","Explorations of relationships between motion, gesture and music; Sonic interaction design",short,poster,remote,5,,posters-1,,nime2025_8.pdf,,,8.jpg,,"Granular Synthesis, Soundscpaes, Plastiglomerates, Neural audio synthesis, Dimensionality reduction",
13,Instant Design: Five Strategies for the use of Generative AI in NIME Ideation Workshops,"This paper presents five strategies for facilitating workshops that incorporate AI text-to-image (TTI) generators in the conceptual design of new musical instruments. Developed through a series of iterative workshops, this approach examines the integration of generative AI (GenAI) within creative processes, with a particular focus on idea generation and the interplay between AI-driven tools and traditional craft-based activities in workshop contexts. The primary study was conducted at the Artificial Intelligence and Musical Creativity (AIMC) Conference 2023 and the paper shares insights from the workshop, including the combination of physical prototyping and GenAI concept design through image creation. The paper emphasises the practical implications of incorporating AI tools into group design fiction workshops and offers five suggestions for facilitators and practitioners. It considers the tensions and opportunities that arise in the collaboration between AI and human creativity, underscoring the importance of iterative feedback and the benefits of clearly defined design briefs within speculative design practices.",Hugh Aynsley (UWE)*; Pete Bennett (University of Bristol); Dave Meckin (Royal College of Art ); Sven Hollowell (University of Bristol); Thomas J. Mitchell (University of the West of England (UWE)),hugh2.aynsley@live.uwe.ac.uk; pete.bennett@bristol.ac.uk; dave.meckin@rca.ac.uk; sven.hollowell@bristol.ac.uk; tom.mitchell@uwe.ac.uk,hugh2.aynsley@live.uwe.ac.uk,paper,Hugh Aynsley; Pete Bennett; Dave Meckin; Sven Hollowell; Thomas J. Mitchell,Machine learning and artificial intelligence in NIMEs,"Accessibility and interfaces for musical expression for people with special needs ; Novel controllers, interfaces or instruments for musical expression",medium,poster,remote,5,,posters-1,,nime2025_13.pdf,,,13.jpg,,"Speculative Design, NIME Workshops, Ideation, Novel Interfaces",
19,A Study to Discover Metrics to Measure Kinesthetic Empathy During Interactive Music Performance,"Kinesthetic empathy is a term used in performance and kinesthetic interaction, defined as the ability of participants to “read, decode and react to each other’s input”.  In prior studies, performers of interactive music self-reported sensing the presence of other musicians. The purpose of this exploratory study was to identify kinesthetic empathy between two individuals in a live electronic performance reported as perceived interactivity. Participants viewed eight videos, both real duets, and spliced solos appearing as real duets, rating each video. The questions guiding this study were: (a) is there a difference in perceived interactivity between the live and spliced duets, (b) is there a relationship between performance rating and perceived interactivity. Results showed a significant difference in the perceived interactivity of the video conditions. Further, the results showed a significant relationship between performance rating and perceived interactivity of the performers. The results suggest that perceived interactivity between performers could be a metric to measure kinesthetic empathy between performers facilitated by an interactive performance system that could be used to objectively measure the effectiveness of design and pedagogical interventions for new interfaces for musical expression.",Ryan Ingebritsen (Ryan Ingebritsen)*; Daniel Evans (West Texas A&M University); Christopher Knowlton (Rush Medical University),waldtiere@gmail.com; devans@wtamu.edu; chrisknowlton@gmail.com,waldtiere@gmail.com,paper,Ryan Ingebritsen; Daniel Evans; Christopher Knowlton,Music-related human-computer interaction,"Entangled NIME: Intertwined, multilayer contexts in NIME research",long,poster,remote,5,,posters-1,,nime2025_19.pdf,,,19.jpg,,"kinesthetic interaction, kinesthetic empathy, interactive electronic music performance, kinesthetic interaction design",
25,Appropriating Technology for Interactive Media in Theatre: Design Strategies and Aesthetic Insights,"This paper investigates interactive media design as a narrative agent in theatrical performance through a practice-based design approach. We clarify the role of the Interaction Director in the production team and examine challenges in appropriating technology for theatrical interaction design. A two-layer workflow is proposed, integrating macro-scale conceptual design with micro-scale cue-to-cue interaction mapping. We address the mutual dependency among creative disciplines, highlighting the collaborative processes necessary to resolve conflicts during the design and rehearsal stages. Furthermore, we adopt a scenographic perspective to analyse how interactive media contributes to dramaturgical storytelling by crafting visual and auditory metaphors. We contextualize interaction design with reader- response theory and the horizon of expectations, demonstrating how interactive media fosters collaborative creativity and expands the narrative potential of theatrical storytelling.","Ruoxi Jia (Goldsmiths, University of London)*; Xuebiao Liu (Goldsmiths, University of London)",ruoxi.jia@outlook.com; liuxuebia@gmail.com,ruoxi.jia@outlook.com,paper,Ruoxi Jia; Xuebiao Liu,Practice-based research approaches/methodologies/criticism,Music-related human-computer interaction ; Technologies or systems for collaborative music-making,long,poster,remote,5,,posters-1,,nime2025_25.pdf,,,25.jpg,,"Interaction, theatre, storytelling",
44,Out-of-Control Feedback Systems and Collaborative Influence with the Instrumentalist Mixer Feedback Transmutation System,"This paper explores the Instrumentalist Mixer Feedback Transmutation (IMFT) system, a modification of the typical no-input mixer paradigm meant for collaborative improvisatory performance (formally called NIMB+)[9]. IMFT occurs when an instrumentalist is patched into a mixing board with feedback loops (a.k.a no-input mixer). The instrumentalist interacts and influences the mixer’s feedback together with another performer operating the mixer. Introducing an instrument into the no-input mixer’s previously closed system creates possibilities for new collaborative interactions between humans and chaotic feedback systems. In this system, a chaotic, out-of-control relationship can be formed where the output of the mixer and the gestures from the mixer performer can be in battle with the input from the instrumentalist and vice-versa. After a brief historical contextualization of mixer feedback, the IMFT system and the complex relationships that form between human and machine are introduced. No-input mixer performance practices are discussed, followed by exploration of a single feedback loop to illustrate some of the mixer’s possible sound worlds and the nature of the instrument. Performance experiences from two recent compositions by the first author, generative open graphic score #1 (2023) and noise ritual (2023), are described in order to explore different performance interactions created by different instrumentalists working with the IMFT system. This practice-based research provides a useful case study examining the entangled relationship between performers and interfaces in feedback-based music systems and how innovative approaches to an established electronic practice can create new perspectives and collaborative opportunities.",Nolan Hildebrand (University of Toronto)*; Timothy Roth (University of Toronto),nolanahildebrand@gmail.com; tim.roth@mail.utoronto.ca,nolanahildebrand@gmail.com,paper,Nolan Hildebrand; Timothy Roth,New music performance paradigms,"Augmented, embedded and hyper instruments ; Entangled NIME: Intertwined, multilayer contexts in NIME research",medium,poster,remote,5,,posters-1,,nime2025_44.pdf,,,44.jpg,,"Feedback, No-input Mixing Board, Augmented instruments, Distortion",
55,Repurposing a Rhythm Accompaniment System for Pipe Organ Performance,"This paper presents an overview of a human-machine collaborative musical performance by Raül Refree utilizing multiple MIDI-enabled pipe organs at Palau Güell, as part of the Organic concert series. Our earlier collaboration focused on live performances using drum generation systems, where generative models captured rhythmic transient structures while ignoring harmonic information. For the organ performance, we required a system capable of generating harmonic sequences in real-time, conditioned on Refree's performance. Instead of developing a comprehensive state-of-the-art model, we integrated a more traditional generative method to convert our pitch-agnostic rhythmic patterns into harmonic sequences. This paper details the development process, the creative and technical considerations behind the final performance, and a reflection on the efficacy and adaptability of the chosen methodology.",Nicholas Evans (Universitat Pompeu Fabra)*; Behzad Haki (Universitat Pompeu Fabra); Sergi Jorda (Universitat Pompeu Fabra),nicholas.evans@upf.edu; behzad.haki@upf.edu; sergi.jorda@upf.edu,nicholas.evans@upf.edu,paper,Nicholas Evans; Behzad Haki; Sergi Jorda,Machine learning in musical performance,Musical mapping strategies ; Performance rendering and generative algorithms,short,poster,remote,5,,posters-1,,nime2025_55.pdf,,,55.jpg,,"Machine Learning, Generative, Performance",
57,Turntable-Based Electronic Music and Embodied Audience Interaction,"Rings… Through Rings transforms archival maps of Hong Kong’s military fortifications into playable surfaces for turntable-based electronic music. Laser-etched discs encode cartographic data, producing sonic textures manipulated through turntables and enhanced by audio techniques like cross-synthesis, concatenative synthesis, and spatialization. Grounded in theories of transcoding, productive agency, and participatory culture, the project reimagines the turntable as a cultural interface, bridging analog heritage with computational sound. This hybrid system blends pre-composed musical structures with real-time audience interaction, allowing participants to alter playback, swap discs, and influence spatial audio. By merging cartography, sound, and participatory design, the work offers a collaborative, multisensory approach to intangible heritage. Future developments include expanded spatial configurations, real-time disc fabrication, and AI integration to deepen engagement and cultural reinterpretation.",Tak Cheung Hui (Hong Kong Metropolitan University)*; Xiaoqiao  Li (Hong Kong Metropolitan University); Yu Chia Kuo (McGill University),tchui@bu.edu; xili@hkmu.edu.hk; yu.kuo@mail.mcgill.ca,tchui@bu.edu,paper,Tak Cheung Hui; Xiaoqiao  Li; Yu Chia Kuo,Interactive sound art and sound installations,Practice-based research approaches/methodologies/criticism,short,poster,remote,5,,posters-1,,nime2025_57.pdf,,,57.jpg,,"Interactive Installation, Digital Fabrication, Interdisciplinary performance, Historical Reinterpretation",
154,LIMITER: A Gamified Interface for Harnessing Just Intonation Systems,"This paper introduces LIMITER, a gamified digital musical instrument for harnessing and performing microtonal and justly intonated sounds. While microtonality in Western music remains a niche and esoteric system that can be difficult both to conceptualize and to perform with, LIMITER presents a novel, easy to pickup interface that utilizes color, geometric transformations, and game-like controls to create a simpler inlet into utilizing these sounds as a means of expression. We report on the background of the development of LIMITER, as well as explain the underlying musical and engineering systems that enable its function. Additionally, we offer a discussion and preliminary evaluation of the creativity-enhancing effects of the interface.",Antonis Christou (MIT)*,antchris@mit.edu,antchris@mit.edu,paper,Antonis Christou,"Novel controllers, interfaces or instruments for musical expression",Musical mapping strategies,medium,poster,remote,5,,posters-1,,nime2025_154.pdf,,,154.jpg,,"Just Intonation, microtonality, gamified interfaces, grid interfaces",
189,PanMan - a modular tangible controller for sound spatialization,"PanMan is a performance-oriented modular midi controller, conceived as a tangible interface for panning multiple sound sources in multichannel audio systems. It consists of four independent control units and a docking base - the modular design allows each of the units to be either physically attached to the base (in which case it might be used as a single controller by a single user) or connected to it via extension cords, allowing up to four users to participate in an interactive sound installation experience or a collaborative performance setting. The physical controls on a single module consist of a joystick/trackball hybrid – a dome-shaped control device designed to be operated with a single finger – and a thumbwheel for additional parameter control, positioned at the edge of the module, allowing for one-handed operation of three parameters. The design facilitates operation by both right- and left-handed users, also allowing a single user to operate two or more controllers simultaneously, controlling a number of parameters at once.",Krzysztof Cybulski (Feliks Nowowiejski Academy of Music in Bydgoszcz)*; Szczepan Busko (Feliks Nowowiejski Academy of Music in Bydgoszcz); Zachariasz Zalewski (Feliks Nowowiejski Academy of Music in Bydgoszcz),kcybulski@pangenerator.com; 4031@mail.amfn.pl; 4160@mail.amfn.pl,kcybulski@pangenerator.com,paper,Krzysztof Cybulski; Szczepan Busko; Zachariasz Zalewski,"Novel controllers, interfaces or instruments for musical expression",Gesture to sound mapping,short,poster,remote,5,,posters-2,,nime2025_189.pdf,,,189.jpg,,"tangible, spatial, controller, interaction",
67,Decoupling Physical and Virtual Spaces in Co-Located Collaborative Mixed Reality Instruments with gRAinyCloud,"Collaborative co-located Mixed Reality musical instruments combine some of the expressive opportunities of 3D interaction and communication and cooperation of physical multi-user instruments. However in existing instruments, the fixed coupling between the virtual and physical environments constrains the affordances brought by Mixed Reality, such as per-musician free navigation in or multi-scale control of virtual structures. We designed gRAinyCloud, as a way to reintegrate these lost affordances to a co-located instrument. It allows for the expressive exploration of a set of sounds represented by a virtual structure of shapes placed in the physical space and shared between musicians. Above all, gRAinyCloud enables each musician to freely manipulate their own viewpoint, changing its scale, position and rotation, effectively decoupling the physical and virtual spaces, and to switch between self, other's and absolute viewpoint while playing. We describe the implementation of this decoupling of spaces and analyse its uses and implications for collective musical expression, by relying on a first-person approach.",Pierrick Uro (McGill University / Université de Lille)*; Florent Berthaut (Université de Lille); Thomas Pietrzak (Université de Lille); Marcelo Wanderley (McGill University),pierrick.uro@gmail.com; florent.berthaut@univ-lille.fr; thomas.pietrzak@univ-lille.fr; marcelo.wanderley@mcgill.ca,pierrick.uro@gmail.com,paper,Pierrick Uro; Florent Berthaut; Thomas Pietrzak; Marcelo Wanderley,"Extended reality environments: augmented, virtual, mixed reality","Novel controllers, interfaces or instruments for musical expression ; Technologies or systems for collaborative music-making",medium,poster,remote,5,,posters-2,,nime2025_67.pdf,,,67.jpg,,"Mixed reality, musical ensembles, decoupling, collaboration, co-located, multi-scale",
73,VentHackz: Exploring the Musicality of Ventilation Systems,"Ventilation systems can be seen as huge examples of interfaces for musical expression, with the potential of merging sound, space, and human interaction. This paper explores conceptual similarities between ventilation systems and wind instruments and explores approaches to “hacking” ventilation systems with components that produce and modify sound. These systems enable the creation of unique sonic and visual experiences by manipulating airflow and making mechanical adjustments. Users can treat ventilation systems as musical interfaces by altering shape, material, and texture or augmenting vents. We call for heightened attention to the sound-making properties of ventilation systems and call for action (#VentHackz) to playfully improve the soundscapes of our indoor environments.",Maham Riaz (University of Oslo)*; Ioannis Theodoridis (Norwegian Academy of Music); Çağrı Erdem (University of Oslo); Alexander Refsum Jensenius (University of Oslo),maham.riaz@imv.uio.no; ioannis.theodoridis@nmh.no; cagrie@ifi.uio.no; a.r.jensenius@imv.uio.no,maham.riaz@imv.uio.no,paper,Maham Riaz; Ioannis Theodoridis; Çağrı Erdem; Alexander Refsum Jensenius,"Historical, theoretical or philosophical discussions about designing or performing with new interfaces","Discussions about the artistic, cultural, and social impact of NIME technology ; Entangled NIME: Intertwined, multilayer contexts in NIME research; Interactive sound art and sound installations",medium,poster,remote,5,,posters-2,,nime2025_73.pdf,,,73.jpg,,"Ventilation Systems, HVAC, Soundscape,  Room Acoustics, Urban Intervention",
125,Creating a Real-Time Responsive Handbalancing Interface with HAND★CS,"This paper introduces HAND★CS, a new interface for interdisci- plinary expression for music, movement, and light. Our interface augments a pedagogical interface for hand-balancing, Haptics- Assisted iNversions Device (HAND), and transforms it into one for artistic expression. It draws upon Licklider’s concept of man- computer symbiosis, specifically the commensalism form of sym- biosis. HAND★CS strives to embody a performance apparatus and system with symbiotic connectivity between performer and interface. This paper discusses the inspiration and background for such a system pulling from the fields of human-computer interaction (HCI), music technology and new interfaces for mu- sical expression (NIME), and circus arts. In addition, it defines the design and implementation, evaluation of the prototype of HAND★CS, and future work.","Linnea Kirby (McGill University)*; Christiana Rose (CirqueIT, LLC); Jeremy Cooperstock (McGill University); Marcelo Wanderley (McGill University)",linnea.kirby@mail.mcgill.ca; christiana.lauren.rose@gmail.com; jer@cim.mcgill.ca; marcelo.wanderley@mcgill.ca,linnea.kirby@mail.mcgill.ca,paper,Linnea Kirby; Christiana Rose; Jeremy Cooperstock; Marcelo Wanderley,"Novel controllers, interfaces or instruments for musical expression","Entangled NIME: Intertwined, multilayer contexts in NIME research; Explorations of relationships between motion, gesture and music; Music-related human-computer interaction",medium,poster,remote,5,,posters-2,,nime2025_125.pdf,,,125.jpg,,"Circus, Handbalancing, Sonification, Performance, Symbiosis",
146,ViVo: Piano Learning Through Visualizing Vocalizations on a Lighted Keyboard,"Vocalization and visualization are recognized as two powerful methods for internalizing music that are effective with beginner and skilled musicians alike. Despite the well-researched benefits of each practice, integrated visualization of vocalizations for instrument learning has seen little attention in the music technology community. This paper introduces the design and implementation of ViVo, a piano learning tool that connects the embodied sense of pitch offered by vocalization with the spatial intuition provided by in situ visualization. ViVo offers two modes: a real-time mode that hears live user vocalizations to concurrently illuminate the corresponding piano keys, and a practice mode that visualizes recorded vocalizations for repeated practice. By providing an integrated system to foster and visualize vocalizations, ViVo aims to leverage the noted benefits of both practices to make learning piano more effective, intuitive, and engaging.",Maya Caren (Independent Researcher)*,mayacaren7@gmail.com,mayacaren7@gmail.com,paper,Maya Caren,Music-related human-computer interaction,"Novel controllers, interfaces or instruments for musical expression",short,poster,remote,5,,posters-2,,nime2025_146.pdf,,,146.jpg,,"Vocalization, Visualization, Learning, Piano, HCI",
155,The EV: An Iterative Journey in Digital-Acoustic String Instrument Augmentation,"Numerous experiments in bowed string augmentation have been undertaken, each reflecting the values and interests of the builder. The EV takes a unique approach, with the convolution of a synthesized and acoustic string signal at the foundation of its design. Through an iterative hardware and software development process, three versions of the instrument have been created, each building toward the goal of a robust compositional and performative platform for exploring the shared boundary of electronic and acoustic sound. Spatialization and physical modeling algorithms have furthered the instrument’s engagement with the interaction between physical and virtual acoustics. This paper examines the iterative design process behind the instrument and its relationship between digital augmentation and acoustic resonance.",Brian Lindgren (Brian Lindgren)*,bklindgren1@gmail.com,bklindgren1@gmail.com,paper,Brian Lindgren,"Augmented, embedded and hyper instruments","Novel controllers, interfaces or instruments for musical expression",long,poster,remote,5,,posters-2,,nime2025_155.pdf,,,155.jpg,,"chordophone, string controller, convolution, cross-synthesis, augmented stringed instrument",
200,Sonicolour: Exploring Colour Control of Sound Synthesis with Interactive Machine Learning,"This paper explores crossmodal mappings of colour to sound. The instrument presented analyses the colour of physical objects via a colour light-to-frequency sensor and maps the corresponding red, green, and blue data values to parameters of a synthesiser. Interactive machine learning is used to facilitate the discovery of new relationships between sound and colour. The role of interactive machine learning is to find unexpected relationships between the visual features of the objects and the sound synthesis. The performance is evaluated by its ability to provide the user with a playful interaction between the visual and tactile exploration of coloured objects, and the generation of synthetic sounds. We conclude by outlining the potential of this approach for musical interaction design and music performance.",Tug F. O'Flaherty (Queen Mary University of London); Luigi Marino (Queen Mary University of London); Charalampos Saitis (Queen Mary University of London)*; Anna Xambó Sedó (Queen Mary University of London),t.f.oflaherty@se24.qmul.ac.uk; l.marino@qmul.ac.uk; c.saitis@qmul.ac.uk; a.xambosedo@qmul.ac.uk,c.saitis@qmul.ac.uk,paper,Tug F. O'Flaherty; Luigi Marino; Charalampos Saitis; Anna Xambó Sedó,"Novel controllers, interfaces or instruments for musical expression",Machine learning and artificial intelligence in NIMEs ; Practice-based research approaches/methodologies/criticism,medium,poster,remote,5,,posters-2,,nime2025_200.pdf,,,200.jpg,,"supervised learning, crossmodal mapping, additive synthesis, synaesthesia",
202,A Spherical Tape Topology for Non-linear Audio Looping,"There have been many physical design formats used in the field of audio recording. As audio has an inherently a linear, time-based structure, these have generally followed logical layouts such as tape, or grooved records and cylinders. This project explores magnetic recording technology and digital analogues for recording and playback that are instead on spherical topology. This instrument expands the concept of the audio loop through a more tangible and randomized approach than traditional record playback techniques of tape, while maintaining a familiarity with historic techniques of audio looping and scrubbing. Through it, one can not only create linear time-loops but blends between different times of the recording non-sequentially. The size and mass of the spheres enhances the performative elements through the physics of inertia. The movement possibilities allow for non-linear circles, circuits, spirals and other patterns of sound not traditionally possible through linear tape or digital loop, including accelerations and decelerations – akin to a turntable, but with greater freedom of direction, thus offering surreal record/playback possibilities.","Kevin Blackistone (Tangible Music Lab, Kunstuniversität Linz, Hauptplatz 6, 4020 Linz)*",kevin@blackistone.com,kevin@blackistone.com,paper,Kevin Blackistone,"Novel controllers, interfaces or instruments for musical expression",Interactive sound art and sound installations ; New music performance paradigms; Sonic interaction design,medium,poster,remote,5,,posters-2,,nime2025_202.pdf,,,202.jpg,,"loops, playback, spheres, non-linearity",
278,A Gesture-Based Approach to Spatialization in Dolby Atmos,"This paper presents a system for the spatialization of sound objects in the Dolby Atmos format, implemented through the integration of an infrared sensor with a chain of three software tools. The setup enables translating hand gestures into spatialization data within the constraints of the Atmos format. The design and parameter mapping are described, along with its usability, strengths, and limitations, as assessed through a preliminary evaluation conducted by the author. Beyond the technical aspects, this article reflects on the author's experience using the system as a mixing engineer and connects these insights to the conceptual framework of related works. This perspective offers a critical reflection on spatialization as a performative practice within studio workflows, highlighting how such devices may be integrated into the multimodal studio environment to introduce new means of interaction in sound spatialization.",Vitor Pinheiro (UFPR)*,pinheiroproducaomusical@gmail.com,pinheiroproducaomusical@gmail.com,paper,Vitor Pinheiro,Gesture to sound mapping,"Explorations of relationships between motion, gesture and music; Novel controllers, interfaces or instruments for musical expression",short,poster,remote,5,,posters-3,,nime2025_278.pdf,,,278.jpg,,"Spatialization, Dolby Atmos, Leap Motion, Music Mixing, Gesture based systems",
208,Exploiting Latency In The Design Of A Networked Music Performance System For Percussive Collective Improvisation,"We present the design, prototype implementation, and informal testing of a distributed web-based networked music performance (NMP) system for collaborative improvisation and experimentation. Influenced by composition and interaction design techniques from a wide range of work on collaborative virtual music environments, rather than treating latency as inherently disruptive to the musical and social engagement that characterizes traditional performance, we incorporate and exploit network delay to facilitate and visualize them, providing a novel approach to creating ""jam session""-like experiences without a separate audience. During sessions, users collaboratively perform semi-improvised music in quasi-real time. The production and interpretation of individual musical gestures (""drum hits"") are visualized in a continuously devised feedback network. The music produced can be treated as a starting point for compositions developed asynchronously, or as complete pieces of music produced live.",Ari Liloia (Carnegie Mellon University)*; Roger Dannenberg (Carnegie Mellon University),aliloia@alumni.cmu.edu; rbd@cs.cmu.edu,aliloia@alumni.cmu.edu,paper,Ari Liloia; Roger Dannenberg,Web-based music performance,"Entangled NIME: Intertwined, multilayer contexts in NIME research; New music performance paradigms; Technologies or systems for collaborative music-making",long,poster,remote,5,,posters-3,,nime2025_208.pdf,,,208.jpg,,"Networked Music Performance, Web-Based Music Systems, Network Latency, Collective Improvisation, Percussion",
226,Two Sonification Methods for the MindCube,"In this work, we explore the musical interface potential of the MindCube, an interactive device designed to study emotions. Embedding diverse sensors and input devices, this interface resembles a fidget cube toy commonly used to help users relieve their stress and anxiety. As such, it is a particularly well-suited controller for musical systems that aim to help with emotion regulation. In this regard, we present two different mappings for the MindCube, with and without AI. With our generative AI mapping, we propose a way to infuse meaning within a latent space and techniques to navigate through it with an external controller. We discuss our results and propose directions for future work.",Fangzheng Liu (MIT Media Lab); Lancelot Blanchard (MIT Media Lab)*; Don D Haddad (MIT Media Lab); Joseph Paradiso (MIT Media Lab),fzliu@media.mit.edu; lancelot@media.mit.edu; ddh@mit.edu; joep@media.mit.edu,lancelot@media.mit.edu,paper,Fangzheng Liu; Lancelot Blanchard; Don D Haddad; Joseph Paradiso,"Novel controllers, interfaces or instruments for musical expression","Gesture to sound mapping; Machine learning and artificial intelligence in NIMEs ; Sensor and actuator technologies, including haptics and force feedback devices",medium,poster,remote,5,,posters-3,,nime2025_226.pdf,,,226.jpg,,"sensors, emotions, generative AI, gesture, controller",
246,Tapping Into a New Paradigm: A Synthetic Strategy for Automatic Drum TapScription,"We introduce Automatic Drum TapScription (ADTS), a novel paradigm for rhythmic interaction consisting of transcribing arbitrarily-timbred taps into drum representations. Our approach targets taps produced on a variety of surfaces without other controlled timbral characteristics other than playing style. Our long-term goal is to enable more accessible and creative percussive exploration but presents significant challenges due to the minimal timbre variation between taps intended to represent different drum classes. To address these challenges, we take the first steps toward achieving ADTS by designing an effective dataset synthesis strategy. This strategy enables new opportuneties for musical expression by considering drumming at a more semantic or functional level as opposed to a simple collection of timbres. We present initial results, comparing three different models: one trained on drum data, another trained on a small dataset of quasi-aligned tapped performances, and another trained on our synthetic dataset. Our synthetic approach shows promise, demonstrating progress in this untapped domain.",André Santos (CISUC - UC)*; Amílcar Cardoso (CISUC - UC); Matthew  E. P. Davies (SiriusXM); Roger B. Dannenberg (CMU),andresantos@dei.uc.pt; amilcar@dei.uc.pt; mepdavies@gmail.com; rbd@andrew.cmu.edu,andresantos@dei.uc.pt,paper,André Santos; Amílcar Cardoso; Matthew  E. P. Davies; Roger B. Dannenberg,"Software frameworks, interface protocols, and data formats, for supporting musical interaction","Machine learning in musical performance; Musical mapping strategies ; Novel controllers, interfaces or instruments for musical expression",long,poster,remote,5,,posters-3,,nime2025_246.pdf,,,246.jpg,,"Automatic Drum Transcription, Rhythmic Interaction, Synthetic Dataset, Musical Expression",
250,Towards a Repository Template for Music Technology Research,"Documenting and sharing research output is essential to construct the critical discourse on new music technology. Documentation feeds the knowledge and the values with which to evaluate and discuss current achievements and musical creations as well as to plan for the future. Besides publishing our research in conferences and journals, sharing research materials and outcomes like software, hardware, instruments, and datasets is important. This allows others to use the latest technology and improve it. For this purpose, the repository is increasingly commonly used by researchers and artists to store and share their works. However, creating repositories does not follow a clear and organised structure like the one we find, for example, in papers. The heterogeneity of repositories makes it hard to use both practically and for analysis. Although the variety and differences of research products in the field of new musical technologies are obvious, we believe that defining repositories with common guidelines could significantly improve the critical discourse in this area. This issue has been discussed at the NIME conference through workshops and papers. In this article, we want to continue this discussion and propose a flexible repository template to organise and present research materials and outcomes in the field of musical technologies research. The article provides a short and focused review of how repositories are currently used at the NIME conference, with special attention to the platforms used. Based on this study, we introduce a repository template that will be applied to case studies. We hope this proposal will encourage further discussion and advancement on this issue and, at the same time, support and facilitate the creation of new repositories.",Alessandro Fiordelmondo (CSC Padova)*; Matteo Spanio (CSC Padova); Patricia Cadavid (Univesity of West of England); Xinran Chen (The Hong Kong University of Science and Technology (Guangzhou)); Sergio Canazza (CSC Padova); Raul Masu (The Hong Kong University of Science and Technology (Guangzhou)),fiordelmondo@dei.unipd.it; spanio@dei.unipd.it; Patricia.Cadavidhinojosa@uwe.ac.uk; xchen805@connect.hkust-gz.edu.cn; canazza@dei.unipd.it; raul@raulmasu.org,fiordelmondo@dei.unipd.it,paper,Alessandro Fiordelmondo; Matteo Spanio; Patricia Cadavid; Xinran Chen; Sergio Canazza; Raul Masu,Technologies or systems for collaborative music-making,"Discussions about the artistic, cultural, and social impact of NIME technology ; Practice-based research approaches/methodologies/criticism ; User studies and evaluations of NIMEs",medium,poster,remote,5,,posters-3,,nime2025_250.pdf,,,250.jpg,,"Documentation, repository, shareability",
263,AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System,"Vocals harmonizers are powerful tools to help solo vocalists enrich their melodies with harmonically supportive voices. These tools exist in various forms, from commercially available pedals and software to custom-built systems, each employing different methods to generate harmonies. Traditional harmonizers often require users to manually specify a key or tonal center, while others allow pitch selection via an external keyboard–both approaches demanding some degree of musical expertise. The AI Harmonizer introduces a novel approach by autonomously generating musically coherent four-part harmonies without requiring prior harmonic input from the user. By integrating state-of-the-art generative AI techniques for pitch detection and voice modeling with custom-trained symbolic music models, our system arranges any vocal melody into rich choral textures. In this paper, we present our methods, explore potential applications in performance and composition, and discuss future directions for real-time implementations. While our system currently operates offline, we believe it represents a significant step toward AI-assisted vocal performance and expressive musical augmentation. We release our implementation on GitHub.",Lancelot Blanchard (MIT Media Lab)*; Cameron Holt (MIT); Joseph Paradiso (MIT Media Lab),lancelot@media.mit.edu; camholt@mit.edu; joep@media.mit.edu,lancelot@media.mit.edu,paper,Lancelot Blanchard; Cameron Holt; Joseph Paradiso,Machine learning and artificial intelligence in NIMEs,"Machine learning in musical performance; Novel controllers, interfaces or instruments for musical expression ; Software frameworks, interface protocols, and data formats, for supporting musical interaction",short,poster,remote,5,,posters-3,,nime2025_263.pdf,,,263.jpg,,"machine learning, generative ai, vocal performance, harmonizer, voice modeling",
282,Synthesizing Music with Logic Gate Networks,"Small digital circuits consisting of basic logic gates (AND, XOR, etc.) are capable of generating surprisingly complex musical output. In this paper, I present physical and web-based interfaces for exploring the space of audio-generating logic gate networks and 'bending' such networks via touch (or mouse) gestures to interfere with their operation and change their output while they are running. This work follows in the vein of bytebeat practices, in which music is generated by short code snippets at the level of individual audio samples, but takes things further by relying on an even lower-level form of computation. In addition to presenting our system, I offer some preliminary analysis of why these logic gate networks tend to produce musical output.",Ian Clester (Georgia Institute of Technology)*,ijc@gatech.edu,ijc@gatech.edu,paper,Ian Clester,"Novel controllers, interfaces or instruments for musical expression",Gesture to sound mapping; Interactive sound art and sound installations ; Web-based music performance,medium,poster,remote,5,,posters-3,,nime2025_282.pdf,,,282.jpg,,"computer music, logic gate synthesis, bytebeat, live coding, circuit bending",
291,Melia: An Expressive Harmonizer at the Limits of AI,"We present Melia, a harmonizer-like digital instrument that explores how common failure modes of machine learning and artificial intelligence (ML/AI) systems can be used in expressive and musical ways. The instrument is anchored by an audio-to-audio neural network trained on a hand-curated dataset to perform pitch-shifting and dynamic filtering. Biased training data and poor out-of-distribution generalization are deliberately leveraged as musical devices and sources of instrument-defining idiosyncrasies. Melia features a custom hardware interface with a MIDI keyboard that polyphonically allocates instances of the model to harmonize live audio input, and integrated hardware controls that manipulate model parameters and various audio effects in real-time. This paper presents an overview of related work, the instrument's hardware and software, and how audio-to-audio AI models might fit into the long-standing tradition of musicians, artists, and instrument-makers finding inspiration in a medium's shortcomings.",Matthew Caren (Massachusetts Institute of Technology)*; Joshua Bennett (Massachusetts Institute of Technology),mcaren@mit.edu; joshuab@mit.edu,mcaren@mit.edu,paper,Matthew Caren; Joshua Bennett,"Novel controllers, interfaces or instruments for musical expression",Machine learning and artificial intelligence in NIMEs ; New music performance paradigms,short,poster,remote,5,,posters-3,,nime2025_291.pdf,,,291.jpg,,"Harmonizer, AI, Voice, NIME",